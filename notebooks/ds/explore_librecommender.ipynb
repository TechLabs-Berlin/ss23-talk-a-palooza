{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51cddc13",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we explore using the library [LibRecommender](https://librecommender.readthedocs.io/) to train a recommendation model on top of the dataset.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "We'll use the [WordBank](http://wordbank.stanford.edu/) datasets.\n",
    "\n",
    "We'll use these datasets the *Full Child-by-Word* and *By-Child Summary* datasets.\n",
    "\n",
    "We won't use the *By-Word Summary* dataset.\n",
    "\n",
    "## Full Child-by-Word\n",
    "\n",
    "Contains the words spoken by each child.\n",
    " \n",
    "It is available in http://wordbank.stanford.edu/data?name=instrument_data.\n",
    "\n",
    "## By-Child Summary\n",
    "\n",
    "Contains information about each child, such as *age* and *gender*.\n",
    " \n",
    "It is available in http://wordbank.stanford.edu/data?name=admin_data.\n",
    "\n",
    "## What is the LibRecommender library\n",
    "\n",
    "The [LibRecommender](https://librecommender.readthedocs.io/) library contains models that can be trained for recommendation tasks.\n",
    "\n",
    "This is what is stated on the library's webpage:\n",
    "\n",
    "> LibRecommender is an easy-to-use recommender system focused on end-to-end recommendation process. It contains a training(libreco) and serving(libserving) module to let users quickly train and deploy different kinds of recommendation models.\n",
    "\n",
    "## Types of models in LibRecommender\n",
    "\n",
    "There are two types of models provided by LibRecommender:\n",
    "\n",
    "1. Pure models: These models take into account only the interactions between users and items. They are based on the [Collaborative Filtering paper](https://ieeexplore.ieee.org/document/7176109).\n",
    "2. Feat models: These models also take into account features of items and users. Examples are (1) the *age* of the user, and (2) genre of a movie.\n",
    "\n",
    "There are multiple algorithms available at the library, such as [LightGCN](https://arxiv.org/pdf/2002.02126.pdf) (a *pure* model) and [Wide & Deep](https://arxiv.org/pdf/1606.07792.pdf) (a *feat* model).\n",
    "\n",
    "# Modeling the task\n",
    "\n",
    "The task to be implemented is to recommend words for a child given a set of words.\n",
    "\n",
    "For example: Given that my child speaks the words \"ball\", \"dad\", and \"mom\", what other words would the system recommend for this child to also learn so that their cognitive development meets the learning curve of words for children.\n",
    "\n",
    "Recommendation models use the terms *item* and *user*, which in our case are respectively *words* and *children*.\n",
    "\n",
    "## Restriction: New children recommendations\n",
    "\n",
    "A characteristic of recommendation systems is that they're trained offline and then used for inference. They calculate all recommended items for a given user. Calculating the recommendations is a slow and compute-intensive process. These systems are usually trained every few days (at night) and are used the following day.\n",
    "\n",
    "The problem is that in our task we want to provide recommendations for a child given a set of words. This set of words won't be in the training dataset, i.e., it is a new user with new items, which the recommendation system hasn't seen yet.\n",
    "\n",
    "Let's give an example so that the problem is more clear:\n",
    "\n",
    "| child id | word  |\n",
    "|----------|-------|\n",
    "| 1        | daddy |\n",
    "| 1        | mommy |\n",
    "| 1        | ball  |\n",
    "| 1        | love  |\n",
    "| 2        | daddy |\n",
    "| 2        | mommy |\n",
    "\n",
    "In this example we can provide recommendations for both children `1` and `2`. However, if we want to give a recommendation for a children that speaks the words `daddy`, `mommy` and `ball`, we can't. This is because the new child speaks a different set of words than all other children. \n",
    "\n",
    "This is similar, but not the same, as the **cold-start problem**. It is different because a cold-start problem means that we don't have data about the new user yet, so we need to \"initialize\" the interactions between `user` and `item`. In our case we **have** data about the new user, however, it is not yet available to the recommendation system because it hasn't yet been trained on the new data.\n",
    "\n",
    "The **solution** we found is to retrain the model with only the new data for the new child. The LibRecommender library makes this possible, so this can be also be thought as \"fine-tuning\". The [documentation](https://librecommender.readthedocs.io/en/latest/user_guide/model_retrain.html) of LibRecommender explains how this can be done. The only \"hack\" we had to do was to create a \"fake\" child (with id `-1`) so that we can \"reuse\" this user when retraining the model.\n",
    "\n",
    "## Modeling with a \"pure\" algorithm\n",
    "\n",
    "Given that these model only the interactions between child/words, it is a matter of preparing the dataset with the right column names and types and feed it to LibRecommender.\n",
    "\n",
    "Under the hood, these algorithms prepare a matrix of (user x item) where the value of each item in this matrix is the \"interaction strength\".\n",
    "\n",
    "In the classic examples of movies recommendations, the value used is the *rating* given by the user to a movie. It could also be used the number of times a user has watched a movie, or a combination of both. In our case, we don't have this information, so we user the value `1.0`.\n",
    "\n",
    "## Modeling with a \"feat\" algorithm\n",
    "\n",
    "TODO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19992dc3",
   "metadata": {},
   "source": [
    "# Notebook setup\n",
    "\n",
    "Installs required libraries and add `import` statements.\n",
    "\n",
    "We'll use Pandas to handle the dataset: data cleaning and data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9eca81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:23:15.229191378Z",
     "start_time": "2023-10-06T20:23:15.228634564Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/bin/python'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T20:23:17.314168871Z",
     "start_time": "2023-10-06T20:23:17.292500849Z"
    }
   },
   "id": "fea42a20a872209a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658ae00c",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T20:28:25.811013560Z",
     "start_time": "2023-10-06T20:23:18.856377177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting LibRecommender\r\n",
      "  Obtaining dependency information for LibRecommender from https://files.pythonhosted.org/packages/c5/0d/7b12f6b4f6136c6d8217a2e6f0ec73129e3d1633327d45300bcd6cadb558/LibRecommender-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached LibRecommender-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\r\n",
      "Collecting tensorflow\r\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/81/16/3aaaf911d8309b9afb29bff97e819c52b011d4ab184c7b01cec92abd018a/tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting pandas\r\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/bc/7e/a9e11bd272e3135108892b6230a115568f477864276181eada3a35d03237/pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Collecting scikit-learn\r\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/af/ad/329a88013936e4372181c0e275c19aa6130b0835876726944b811af5a856/scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting pyarrow\r\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/49/db/0a40d2a5b2382c77536479894ce2900e5f4c40251681a72d397ba6430f8d/pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata\r\n",
      "  Using cached pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: ipywidgets in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (8.1.1)\r\n",
      "Collecting torch\r\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d3/fa/93f3ef65dee8947d8f54eb876ab57a8b019845a45dc07546e2ac214da97b/torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl.metadata\r\n",
      "  Downloading torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting gensim>=4.0.0 (from LibRecommender)\r\n",
      "  Obtaining dependency information for gensim>=4.0.0 from https://files.pythonhosted.org/packages/7b/ef/d559c7daebb2f00b881575551b23866ebcbf6eeaf33393d692c7f46d0983/gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\r\n",
      "Collecting tqdm (from LibRecommender)\r\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\r\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\r\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\r\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\r\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\r\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\r\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\r\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\r\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\r\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/4f/79/8e6e05bc4954ebdb8b9c587f780a11f28790585798bd15a8e4870cfc02bc/h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\r\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\r\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/7b/be/4b211a4e432502c432e3077aa66b0d64f6d7cb4c36613d65c49d9b799919/ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting numpy>=1.23.5 (from tensorflow)\r\n",
      "  Obtaining dependency information for numpy>=1.23.5 from https://files.pythonhosted.org/packages/75/cd/7ae0f2cd3fc68aea6cfb2b7e523842e1fa953adb38efabc110d27ba6e423/numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\r\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\r\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "Requirement already satisfied: packaging in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from tensorflow) (23.2)\r\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\r\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\r\n",
      "Requirement already satisfied: setuptools in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from tensorflow) (68.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\r\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from tensorflow) (4.8.0)\r\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\r\n",
      "  Using cached wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\r\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/49/49/fa8224d5da4c81c958503c159bae176cae329f8704bef881d8fd7d99a180/tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\r\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\r\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\r\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/d1/a1/adf44cb808bcda1997d8afb3033b4fd503f6f5e89a6d3eeb454cb84c8abc/grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\r\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\r\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\r\n",
      "  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\r\n",
      "  Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow)\r\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\r\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Collecting pytz>=2020.1 (from pandas)\r\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\r\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.1 (from pandas)\r\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\r\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\r\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/88/8c/9d1f74196c296046af1f20e6d3fc7fbb27387282315e1643f450bba14329/scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\r\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\r\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\r\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\r\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipywidgets) (0.1.4)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipywidgets) (8.16.1)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipywidgets) (5.11.2)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.9)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.9)\r\n",
      "Collecting filelock (from torch)\r\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\r\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting sympy (from torch)\r\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "Collecting networkx (from torch)\r\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\r\n",
      "Requirement already satisfied: jinja2 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from torch) (3.1.2)\r\n",
      "Collecting fsspec (from torch)\r\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\r\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\r\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\r\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m209.8/209.8 MB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting triton==2.1.0 (from torch)\r\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/d1/5a/e5811fcc8fc6703be39eb157af6224eaa3b628a42008df93b87e23eb9731/triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\r\n",
      "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\r\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\r\n",
      "Collecting smart-open>=1.8.1 (from gensim>=4.0.0->LibRecommender)\r\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\r\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: backcall in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\r\n",
      "Requirement already satisfied: stack-data in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: exceptiongroup in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.3)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\r\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\r\n",
      "  Using cached google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\r\n",
      "  Downloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\r\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\r\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/b6/a5/54b01f663d60d5334f6c9c87c26274e94617a4fd463d812463626423b10d/werkzeug-3.0.0-py3-none-any.whl.metadata\r\n",
      "  Using cached werkzeug-3.0.0-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\r\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\r\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (6.8.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\r\n",
      "Requirement already satisfied: pure-eval in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow) (3.17.0)\r\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\r\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\r\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\r\n",
      "Using cached LibRecommender-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "Using cached tensorflow-2.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\r\n",
      "Using cached ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\r\n",
      "Using cached pandas-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\r\n",
      "Using cached scikit_learn-1.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\r\n",
      "Using cached pyarrow-13.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\r\n",
      "Downloading torch-2.1.0-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m670.2/670.2 MB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hDownloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.3/89.3 MB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached absl_py-2.0.0-py3-none-any.whl (130 kB)\r\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\r\n",
      "Using cached gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\r\n",
      "Using cached grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\r\n",
      "Using cached h5py-3.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\r\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\r\n",
      "Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\r\n",
      "Using cached numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\r\n",
      "Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m311.6/311.6 kB\u001B[0m \u001B[31m12.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\r\n",
      "Using cached scipy-1.11.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\r\n",
      "Using cached tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\r\n",
      "Using cached tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\r\n",
      "Using cached tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\r\n",
      "Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\r\n",
      "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m173.4/173.4 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\r\n",
      "Downloading Markdown-3.5-py3-none-any.whl (101 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m101.7/101.7 kB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached smart_open-6.4.0-py3-none-any.whl (57 kB)\r\n",
      "Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\r\n",
      "Using cached werkzeug-3.0.0-py3-none-any.whl (226 kB)\r\n",
      "Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.2/20.2 MB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\r\n",
      "Installing collected packages: pytz, mpmath, libclang, flatbuffers, wrapt, werkzeug, tzdata, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sympy, smart-open, pyasn1, protobuf, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, keras, joblib, grpcio, google-pasta, gast, fsspec, filelock, cachetools, astunparse, absl-py, triton, scipy, rsa, requests-oauthlib, pyasn1-modules, pyarrow, pandas, opt-einsum, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ml-dtypes, markdown, h5py, scikit-learn, nvidia-cusolver-cu12, google-auth, gensim, torch, LibRecommender, google-auth-oauthlib, tensorboard, tensorflow\r\n",
      "Successfully installed LibRecommender-1.3.0 absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 filelock-3.12.4 flatbuffers-23.5.26 fsspec-2023.9.2 gast-0.5.4 gensim-4.3.2 google-auth-2.23.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 h5py-3.9.0 joblib-1.3.2 keras-2.14.0 libclang-16.0.6 markdown-3.5 ml-dtypes-0.2.0 mpmath-1.3.0 networkx-3.1 numpy-1.26.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-2.1.1 protobuf-4.24.4 pyarrow-13.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pytz-2023.3.post1 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.3.1 scipy-1.11.3 smart-open-6.4.0 sympy-1.12 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 threadpoolctl-3.2.0 torch-2.1.0 tqdm-4.66.1 triton-2.1.0 tzdata-2023.3 werkzeug-3.0.0 wrapt-1.14.1\r\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install LibRecommender tensorflow pandas scikit-learn pyarrow ipywidgets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d439aa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:28:25.838697616Z",
     "start_time": "2023-10-06T20:28:25.814213021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'3.9.18 (main, Oct  1 2023, 17:59:15) \\n[GCC 13.2.1 20230801]'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dae3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:28:35.959072815Z",
     "start_time": "2023-10-06T20:28:35.906326089Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b567a",
   "metadata": {},
   "source": [
    "Load the Full Child-by-Word dataset.\n",
    " \n",
    "We'll use the parquet format so that it is smaller on disk. This dataset size in CSV is ~500MB, while in Parquet it is ~5MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087a0c65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:28:38.071108059Z",
     "start_time": "2023-10-06T20:28:38.060386479Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_df(dataset_filename: str) -> pd.DataFrame:\n",
    "    file = Path(dataset_filename)\n",
    "    dataset_file_parquet = file.with_suffix(\".parquet\")\n",
    "    if not dataset_file_parquet.exists():\n",
    "        dataset_file_csv = file.with_suffix(\".csv\")\n",
    "        if not dataset_file_csv.exists():\n",
    "            # This URL might not work for everyone. If it doesn't, then download it manually by following the link available at the Introduction of this notebook.\n",
    "            !curl -o {dataset_file_csv} http://52.26.82.213/instrument_data/_w_8a927d8be09f64164925949e01d5961a35c4a2199f9395eb/session/620431f5bceb2a1e6a87dc53a0ab3c20/download/download_data?w=8a927d8be09f64164925949e01d5961a35c4a2199f9395eb\n",
    "        df = pd.read_csv(dataset_file_csv) \n",
    "        df.to_parquet(dataset_file_parquet)\n",
    "    else:\n",
    "        df = pd.read_parquet(dataset_file_parquet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ff28dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:36:55.296826310Z",
     "start_time": "2023-10-06T20:36:54.336460659Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_df(\"wordbank_instrument_data_full_child_by_word_englishAmerican_WS.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2379c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:36:55.352304659Z",
     "start_time": "2023-10-06T20:36:55.308357268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         downloaded  data_id   item_kind category   item_id  \\\n0        2023-09-07   245518        word   sounds    item_1   \n1        2023-09-07   245518        word   sounds    item_2   \n2        2023-09-07   245518        word   sounds    item_3   \n3        2023-09-07   245518        word   sounds    item_4   \n4        2023-09-07   245518        word   sounds    item_5   \n...             ...      ...         ...      ...       ...   \n6057992  2023-09-07   255023  complexity     None  item_793   \n6057993  2023-09-07   255023  complexity     None  item_794   \n6057994  2023-09-07   255023  complexity     None  item_795   \n6057995  2023-09-07   255023  complexity     None  item_796   \n6057996  2023-09-07   255023  complexity     None  item_797   \n\n                                      item_definition  \\\n0                                             baa baa   \n1                                           choo choo   \n2                                      cockadoodledoo   \n3                                                grrr   \n4                                                meow   \n...                                               ...   \n6057992                    lookit / lookit what I got   \n6057993  where's my dolly / where's my dolly name Sam   \n6057994          we made this / me and Paul made this   \n6057995             I sing song / I sing song for you   \n6057996       baby crying / baby crying cuz she's sad   \n\n                                        english_gloss       uni_lemma  \\\n0                                             baa baa         baa baa   \n1                                           choo choo       choo choo   \n2                                      cockadoodledoo  cockadoodledoo   \n3                                                grrr            grrr   \n4                                                meow            meow   \n...                                               ...             ...   \n6057992                    lookit / lookit what I got            None   \n6057993  where's my dolly / where's my dolly name Sam            None   \n6057994          we made this / me and Paul made this            None   \n6057995             I sing song / I sing song for you            None   \n6057996       baby crying / baby crying cuz she's sad            None   \n\n         child_id  age     value  \n0               1   28  produces  \n1               1   28      None  \n2               1   28      None  \n3               1   28  produces  \n4               1   28      None  \n...           ...  ...       ...  \n6057992     86615   23    simple  \n6057993     86615   23    simple  \n6057994     86615   23    simple  \n6057995     86615   23    simple  \n6057996     86615   23    simple  \n\n[6057997 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>downloaded</th>\n      <th>data_id</th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_id</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>1</td>\n      <td>28</td>\n      <td>produces</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_2</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>1</td>\n      <td>28</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_3</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>1</td>\n      <td>28</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_4</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>1</td>\n      <td>28</td>\n      <td>produces</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_5</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>1</td>\n      <td>28</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6057992</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>complexity</td>\n      <td>None</td>\n      <td>item_793</td>\n      <td>lookit / lookit what I got</td>\n      <td>lookit / lookit what I got</td>\n      <td>None</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>simple</td>\n    </tr>\n    <tr>\n      <th>6057993</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>complexity</td>\n      <td>None</td>\n      <td>item_794</td>\n      <td>where's my dolly / where's my dolly name Sam</td>\n      <td>where's my dolly / where's my dolly name Sam</td>\n      <td>None</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>simple</td>\n    </tr>\n    <tr>\n      <th>6057994</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>complexity</td>\n      <td>None</td>\n      <td>item_795</td>\n      <td>we made this / me and Paul made this</td>\n      <td>we made this / me and Paul made this</td>\n      <td>None</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>simple</td>\n    </tr>\n    <tr>\n      <th>6057995</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>complexity</td>\n      <td>None</td>\n      <td>item_796</td>\n      <td>I sing song / I sing song for you</td>\n      <td>I sing song / I sing song for you</td>\n      <td>None</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>simple</td>\n    </tr>\n    <tr>\n      <th>6057996</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>complexity</td>\n      <td>None</td>\n      <td>item_797</td>\n      <td>baby crying / baby crying cuz she's sad</td>\n      <td>baby crying / baby crying cuz she's sad</td>\n      <td>None</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>simple</td>\n    </tr>\n  </tbody>\n</table>\n<p>6057997 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "         downloaded  data_id item_kind          category   item_id  \\\n0        2023-09-07   245518      word            sounds    item_1   \n1        2023-09-07   245519      word            sounds    item_1   \n2        2023-09-07   245520      word            sounds    item_1   \n3        2023-09-07   245521      word            sounds    item_1   \n4        2023-09-07   245522      word            sounds    item_1   \n...             ...      ...       ...               ...       ...   \n4963448  2023-09-07   255019      word  connecting_words  item_680   \n4963449  2023-09-07   255020      word  connecting_words  item_680   \n4963450  2023-09-07   255021      word  connecting_words  item_680   \n4963451  2023-09-07   255022      word  connecting_words  item_680   \n4963452  2023-09-07   255023      word  connecting_words  item_680   \n\n        item_definition english_gloss uni_lemma  child_id  age     value  \\\n0               baa baa       baa baa   baa baa         1   28  produces   \n1               baa baa       baa baa   baa baa         2   22      None   \n2               baa baa       baa baa   baa baa         3   26  produces   \n3               baa baa       baa baa   baa baa         4   27  produces   \n4               baa baa       baa baa   baa baa         5   19  produces   \n...                 ...           ...       ...       ...  ...       ...   \n4963448            then          then      then     86611   22      None   \n4963449            then          then      then     86612   29  produces   \n4963450            then          then      then     86613   22      None   \n4963451            then          then      then     86614   28      None   \n4963452            then          then      then     86615   23      None   \n\n                       wordBankId     word  \n0        651de3dbf3a9be0887dd1d86  baa baa  \n1        651de3dbf3a9be0887dd1d86  baa baa  \n2        651de3dbf3a9be0887dd1d86  baa baa  \n3        651de3dbf3a9be0887dd1d86  baa baa  \n4        651de3dbf3a9be0887dd1d86  baa baa  \n...                           ...      ...  \n4963448  651de3dbf3a9be0887dd202d     then  \n4963449  651de3dbf3a9be0887dd202d     then  \n4963450  651de3dbf3a9be0887dd202d     then  \n4963451  651de3dbf3a9be0887dd202d     then  \n4963452  651de3dbf3a9be0887dd202d     then  \n\n[4963453 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>downloaded</th>\n      <th>data_id</th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_id</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>value</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-09-07</td>\n      <td>245518</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>1</td>\n      <td>28</td>\n      <td>produces</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-09-07</td>\n      <td>245519</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>2</td>\n      <td>22</td>\n      <td>None</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-09-07</td>\n      <td>245520</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>3</td>\n      <td>26</td>\n      <td>produces</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-09-07</td>\n      <td>245521</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>4</td>\n      <td>27</td>\n      <td>produces</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-09-07</td>\n      <td>245522</td>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>5</td>\n      <td>19</td>\n      <td>produces</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4963448</th>\n      <td>2023-09-07</td>\n      <td>255019</td>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86611</td>\n      <td>22</td>\n      <td>None</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963449</th>\n      <td>2023-09-07</td>\n      <td>255020</td>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86612</td>\n      <td>29</td>\n      <td>produces</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963450</th>\n      <td>2023-09-07</td>\n      <td>255021</td>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86613</td>\n      <td>22</td>\n      <td>None</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963451</th>\n      <td>2023-09-07</td>\n      <td>255022</td>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86614</td>\n      <td>28</td>\n      <td>None</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963452</th>\n      <td>2023-09-07</td>\n      <td>255023</td>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86615</td>\n      <td>23</td>\n      <td>None</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>4963453 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the training dataset with the words we want to use (wordbanks.json)\n",
    "\n",
    "df_wordbanks = pd.read_json(\"wordbanks.json\")\n",
    "df_wordbanks[\"wordBankId\"] = df_wordbanks[\"_id\"].apply(lambda d: d[\"$oid\"])\n",
    "df_wordbanks.rename(columns={\"name\": \"word\"}, inplace=True)\n",
    "df_wordbanks = df_wordbanks[[\"wordBankId\", \"word\"]]\n",
    "df = df.merge(df_wordbanks, left_on=\"item_definition\", right_on=\"word\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T20:36:56.899204765Z",
     "start_time": "2023-10-06T20:36:55.351765177Z"
    }
   },
   "id": "69faf4b247f0f90c"
  },
  {
   "cell_type": "markdown",
   "id": "0b4c03a8",
   "metadata": {},
   "source": [
    "Drop some columns which we will not use because they're metadata, so we're not interested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fde5bd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:37:47.220743470Z",
     "start_time": "2023-10-06T20:37:46.642305941Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['downloaded', 'data_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdownloaded\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df\n",
      "File \u001B[0;32m~/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/pandas/core/frame.py:5347\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5200\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5201\u001B[0m     labels: IndexLabel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5208\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5210\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5211\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[1;32m   5212\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5345\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[1;32m   5346\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 5347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5349\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5350\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5351\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5353\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5354\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5355\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4711\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4709\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   4710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4711\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4713\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   4714\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[0;32m~/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/pandas/core/generic.py:4753\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4751\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m   4752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4753\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4754\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[1;32m   4756\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[1;32m   4757\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:6992\u001B[0m, in \u001B[0;36mIndex.drop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   6990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m   6991\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 6992\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask]\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6993\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[1;32m   6994\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['downloaded', 'data_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop([\"downloaded\", \"data_id\"], axis=\"columns\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e70fef",
   "metadata": {},
   "source": [
    "The `value` column, when `None`, means that child doesn't do anything with the specific word.\n",
    " \n",
    "We'll discard the rows where this column is empty and the entire column because it doesn't add anything of value:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c017bb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:37:48.098553563Z",
     "start_time": "2023-10-06T20:37:47.671908387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind          category   item_id item_definition english_gloss  \\\n0            word            sounds    item_1         baa baa       baa baa   \n2            word            sounds    item_1         baa baa       baa baa   \n3            word            sounds    item_1         baa baa       baa baa   \n4            word            sounds    item_1         baa baa       baa baa   \n5            word            sounds    item_1         baa baa       baa baa   \n...           ...               ...       ...             ...           ...   \n4963438      word  connecting_words  item_680            then          then   \n4963441      word  connecting_words  item_680            then          then   \n4963444      word  connecting_words  item_680            then          then   \n4963446      word  connecting_words  item_680            then          then   \n4963449      word  connecting_words  item_680            then          then   \n\n        uni_lemma  child_id  age                wordBankId     word  \n0         baa baa         1   28  651de3dbf3a9be0887dd1d86  baa baa  \n2         baa baa         3   26  651de3dbf3a9be0887dd1d86  baa baa  \n3         baa baa         4   27  651de3dbf3a9be0887dd1d86  baa baa  \n4         baa baa         5   19  651de3dbf3a9be0887dd1d86  baa baa  \n5         baa baa         6   30  651de3dbf3a9be0887dd1d86  baa baa  \n...           ...       ...  ...                       ...      ...  \n4963438      then     86601   29  651de3dbf3a9be0887dd202d     then  \n4963441      then     86604   30  651de3dbf3a9be0887dd202d     then  \n4963444      then     86607   25  651de3dbf3a9be0887dd202d     then  \n4963446      then     86609   29  651de3dbf3a9be0887dd202d     then  \n4963449      then     86612   29  651de3dbf3a9be0887dd202d     then  \n\n[2013822 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_id</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>1</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>3</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>4</td>\n      <td>27</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>5</td>\n      <td>19</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>item_1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>6</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4963438</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86601</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963441</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86604</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963444</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86607</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963446</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86609</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963449</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>item_680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86612</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>2013822 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=\"value\", inplace=True)\n",
    "df.drop(columns=[\"value\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0dd39e",
   "metadata": {},
   "source": [
    "The `item_id` has this prefix `item_` which doesn't help and makes the data confusing. Let's just turn it into an `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e24bb7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:37:55.429355529Z",
     "start_time": "2023-10-06T20:37:54.345249232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind          category  item_id item_definition english_gloss  \\\n0            word            sounds        1         baa baa       baa baa   \n2            word            sounds        1         baa baa       baa baa   \n3            word            sounds        1         baa baa       baa baa   \n4            word            sounds        1         baa baa       baa baa   \n5            word            sounds        1         baa baa       baa baa   \n...           ...               ...      ...             ...           ...   \n4963438      word  connecting_words      680            then          then   \n4963441      word  connecting_words      680            then          then   \n4963444      word  connecting_words      680            then          then   \n4963446      word  connecting_words      680            then          then   \n4963449      word  connecting_words      680            then          then   \n\n        uni_lemma  child_id  age                wordBankId     word  \n0         baa baa         1   28  651de3dbf3a9be0887dd1d86  baa baa  \n2         baa baa         3   26  651de3dbf3a9be0887dd1d86  baa baa  \n3         baa baa         4   27  651de3dbf3a9be0887dd1d86  baa baa  \n4         baa baa         5   19  651de3dbf3a9be0887dd1d86  baa baa  \n5         baa baa         6   30  651de3dbf3a9be0887dd1d86  baa baa  \n...           ...       ...  ...                       ...      ...  \n4963438      then     86601   29  651de3dbf3a9be0887dd202d     then  \n4963441      then     86604   30  651de3dbf3a9be0887dd202d     then  \n4963444      then     86607   25  651de3dbf3a9be0887dd202d     then  \n4963446      then     86609   29  651de3dbf3a9be0887dd202d     then  \n4963449      then     86612   29  651de3dbf3a9be0887dd202d     then  \n\n[2013822 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_id</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>1</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>3</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>4</td>\n      <td>27</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>5</td>\n      <td>19</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>6</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4963438</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86601</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963441</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86604</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963444</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86607</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963446</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86609</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963449</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86612</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>2013822 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if df.dtypes[\"item_id\"] != int:\n",
    "    df[\"item_id\"] = df[\"item_id\"].apply(lambda v: int(v.removeprefix(\"item_\")))\n",
    "assert df.dtypes[\"item_id\"] == int\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f178e9",
   "metadata": {},
   "source": [
    "# By-Child dataset\n",
    "\n",
    "Enhance the dataframe with children information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "faddccee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:38:03.452149785Z",
     "start_time": "2023-10-06T20:38:03.428453790Z"
    }
   },
   "outputs": [],
   "source": [
    "df_children = load_df(\"wordbank_administration_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18869b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:38:03.938239190Z",
     "start_time": "2023-10-06T20:38:03.894554922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       downloaded               language form dataset_name  child_id  age  \\\n0      2023-09-28               Croatian   WG         CLEX     18186   13   \n1      2023-09-28               Croatian   WG         CLEX     18187   16   \n2      2023-09-28               Croatian   WG         CLEX     18188    9   \n3      2023-09-28               Croatian   WG         CLEX     18189   12   \n4      2023-09-28               Croatian   WG         CLEX     18190   12   \n...           ...                    ...  ...          ...       ...  ...   \n90459  2023-09-28  Portuguese (European)   WG       Cadime     66175   14   \n90460  2023-09-28  Portuguese (European)   WG       Cadime     66176   15   \n90461  2023-09-28  Portuguese (European)   WG       Cadime     66177   15   \n90462  2023-09-28  Portuguese (European)   WG       Cadime     66178   15   \n90463  2023-09-28  Portuguese (European)   WG       Cadime     66179   15   \n\n       comprehension  production  is_norming birth_order  ...  race     sex  \\\n0                293          88        True        None  ...  None  Female   \n1                122          12        True        None  ...  None    Male   \n2                  3           0        True        None  ...  None  Female   \n3                  0           0        True        None  ...  None  Female   \n4                 44           0        True        None  ...  None  Female   \n...              ...         ...         ...         ...  ...   ...     ...   \n90459            162          11       False        None  ...  None  Female   \n90460            314          47       False        None  ...  None  Female   \n90461            103          15       False        None  ...  None  Female   \n90462            256          24       False        None  ...  None    Male   \n90463            117          62       False        None  ...  None    Male   \n\n      birth_weight born_early_or_late  gestational_age  zygosity  \\\n0              NaN                NaN              NaN       NaN   \n1              NaN                NaN              NaN       NaN   \n2              NaN                NaN              NaN       NaN   \n3              NaN                NaN              NaN       NaN   \n4              NaN                NaN              NaN       NaN   \n...            ...                ...              ...       ...   \n90459          NaN                NaN              NaN       NaN   \n90460          NaN                NaN              NaN       NaN   \n90461          NaN                NaN              NaN       NaN   \n90462          NaN                NaN              NaN       NaN   \n90463          NaN                NaN              NaN       NaN   \n\n       language_exposures  health_conditions  monolingual  \\\n0                     NaN                NaN         True   \n1                     NaN                NaN         True   \n2                     NaN                NaN         True   \n3                     NaN                NaN         True   \n4                     NaN                NaN         True   \n...                   ...                ...          ...   \n90459                 NaN                NaN         True   \n90460                 NaN                NaN         True   \n90461                 NaN                NaN         True   \n90462                 NaN                NaN         True   \n90463                 NaN                NaN         True   \n\n       typically_developing  \n0                      True  \n1                      True  \n2                      True  \n3                      True  \n4                      True  \n...                     ...  \n90459                  True  \n90460                  True  \n90461                  True  \n90462                  True  \n90463                  True  \n\n[90464 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>downloaded</th>\n      <th>language</th>\n      <th>form</th>\n      <th>dataset_name</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>comprehension</th>\n      <th>production</th>\n      <th>is_norming</th>\n      <th>birth_order</th>\n      <th>...</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>birth_weight</th>\n      <th>born_early_or_late</th>\n      <th>gestational_age</th>\n      <th>zygosity</th>\n      <th>language_exposures</th>\n      <th>health_conditions</th>\n      <th>monolingual</th>\n      <th>typically_developing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-09-28</td>\n      <td>Croatian</td>\n      <td>WG</td>\n      <td>CLEX</td>\n      <td>18186</td>\n      <td>13</td>\n      <td>293</td>\n      <td>88</td>\n      <td>True</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-09-28</td>\n      <td>Croatian</td>\n      <td>WG</td>\n      <td>CLEX</td>\n      <td>18187</td>\n      <td>16</td>\n      <td>122</td>\n      <td>12</td>\n      <td>True</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Male</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-09-28</td>\n      <td>Croatian</td>\n      <td>WG</td>\n      <td>CLEX</td>\n      <td>18188</td>\n      <td>9</td>\n      <td>3</td>\n      <td>0</td>\n      <td>True</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-09-28</td>\n      <td>Croatian</td>\n      <td>WG</td>\n      <td>CLEX</td>\n      <td>18189</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-09-28</td>\n      <td>Croatian</td>\n      <td>WG</td>\n      <td>CLEX</td>\n      <td>18190</td>\n      <td>12</td>\n      <td>44</td>\n      <td>0</td>\n      <td>True</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>90459</th>\n      <td>2023-09-28</td>\n      <td>Portuguese (European)</td>\n      <td>WG</td>\n      <td>Cadime</td>\n      <td>66175</td>\n      <td>14</td>\n      <td>162</td>\n      <td>11</td>\n      <td>False</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>90460</th>\n      <td>2023-09-28</td>\n      <td>Portuguese (European)</td>\n      <td>WG</td>\n      <td>Cadime</td>\n      <td>66176</td>\n      <td>15</td>\n      <td>314</td>\n      <td>47</td>\n      <td>False</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>90461</th>\n      <td>2023-09-28</td>\n      <td>Portuguese (European)</td>\n      <td>WG</td>\n      <td>Cadime</td>\n      <td>66177</td>\n      <td>15</td>\n      <td>103</td>\n      <td>15</td>\n      <td>False</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>90462</th>\n      <td>2023-09-28</td>\n      <td>Portuguese (European)</td>\n      <td>WG</td>\n      <td>Cadime</td>\n      <td>66178</td>\n      <td>15</td>\n      <td>256</td>\n      <td>24</td>\n      <td>False</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Male</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>90463</th>\n      <td>2023-09-28</td>\n      <td>Portuguese (European)</td>\n      <td>WG</td>\n      <td>Cadime</td>\n      <td>66179</td>\n      <td>15</td>\n      <td>117</td>\n      <td>62</td>\n      <td>False</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Male</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>90464 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea01d6f",
   "metadata": {},
   "source": [
    "### Words dataframe\n",
    "\n",
    "Create a dataframe only with the words/sentences contents. This way we have an index of words which we can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7573f974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:39:11.266919474Z",
     "start_time": "2023-10-06T20:39:11.208518803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind          category item_definition   english_gloss  \\\nitem_id                                                               \n1            word            sounds         baa baa         baa baa   \n2            word            sounds       choo choo       choo choo   \n3            word            sounds  cockadoodledoo  cockadoodledoo   \n4            word            sounds            grrr            grrr   \n5            word            sounds            meow            meow   \n...           ...               ...             ...             ...   \n676          word  connecting_words         because         because   \n677          word  connecting_words             but             but   \n678          word  connecting_words              if              if   \n679          word  connecting_words              so              so   \n680          word  connecting_words            then            then   \n\n              uni_lemma                wordBankId            word  \nitem_id                                                            \n1               baa baa  651de3dbf3a9be0887dd1d86         baa baa  \n2             choo choo  651de3dbf3a9be0887dd1d87       choo choo  \n3        cockadoodledoo  651de3dbf3a9be0887dd1d88  cockadoodledoo  \n4                  grrr  651de3dbf3a9be0887dd1d89            grrr  \n5                  meow  651de3dbf3a9be0887dd1d8a            meow  \n...                 ...                       ...             ...  \n676             because  651de3dbf3a9be0887dd2029         because  \n677                 but  651de3dbf3a9be0887dd202a             but  \n678                  if  651de3dbf3a9be0887dd202b              if  \n679                  so  651de3dbf3a9be0887dd202c              so  \n680                then  651de3dbf3a9be0887dd202d            then  \n\n[653 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>651de3dbf3a9be0887dd1d87</td>\n      <td>choo choo</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>651de3dbf3a9be0887dd1d88</td>\n      <td>cockadoodledoo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>651de3dbf3a9be0887dd1d89</td>\n      <td>grrr</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>651de3dbf3a9be0887dd1d8a</td>\n      <td>meow</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>676</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>because</td>\n      <td>because</td>\n      <td>because</td>\n      <td>651de3dbf3a9be0887dd2029</td>\n      <td>because</td>\n    </tr>\n    <tr>\n      <th>677</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>but</td>\n      <td>but</td>\n      <td>but</td>\n      <td>651de3dbf3a9be0887dd202a</td>\n      <td>but</td>\n    </tr>\n    <tr>\n      <th>678</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>if</td>\n      <td>if</td>\n      <td>if</td>\n      <td>651de3dbf3a9be0887dd202b</td>\n      <td>if</td>\n    </tr>\n    <tr>\n      <th>679</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>so</td>\n      <td>so</td>\n      <td>so</td>\n      <td>651de3dbf3a9be0887dd202c</td>\n      <td>so</td>\n    </tr>\n    <tr>\n      <th>680</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>653 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = df.drop_duplicates(\"item_id\").drop([\"child_id\", \"age\"], axis=\"columns\")\n",
    "df_words.set_index(\"item_id\", inplace=True)\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d444d",
   "metadata": {},
   "source": [
    "# The `item_kind` column\n",
    "\n",
    "This refers the type of the entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74510cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:39:38.405336037Z",
     "start_time": "2023-10-06T20:39:38.361971370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "item_kind\nword    2013822\nName: count, dtype: int64"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many entries for each child do we have\n",
    "df[\"item_kind\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1f624e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:39:39.708976158Z",
     "start_time": "2023-10-06T20:39:39.676789566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind    category item_definition english_gloss uni_lemma  \\\nitem_id                                                                 \n97           word  food_drink          cereal        cereal    cereal   \n\n                       wordBankId    word  \nitem_id                                    \n97       651de3dbf3a9be0887dd1de6  cereal  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97</th>\n      <td>word</td>\n      <td>food_drink</td>\n      <td>cereal</td>\n      <td>cereal</td>\n      <td>cereal</td>\n      <td>651de3dbf3a9be0887dd1de6</td>\n      <td>cereal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some of the data\n",
    "df_words.groupby(\"item_kind\").sample(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58fd3b4",
   "metadata": {},
   "source": [
    "As can be seen, the entries where `item_kind` is one of:\n",
    "\n",
    "- `combine`\n",
    "- `word_endings`\n",
    "- `word_endings_nouns`\n",
    "- `word_endings_verbs`\n",
    "- `how_use_words`\n",
    "\n",
    "These entries are confusing from a \"standalone word\" point of view. They don't fit our task of recommending words. For example: We can't use `sockses` because it is not a \"word\", it is the ending of a word.\n",
    "\n",
    "Let's drop entries from our `df` with these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e268bb4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:39:57.555025423Z",
     "start_time": "2023-10-06T20:39:57.332314721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind          category  item_id item_definition english_gloss  \\\n0            word            sounds        1         baa baa       baa baa   \n2            word            sounds        1         baa baa       baa baa   \n3            word            sounds        1         baa baa       baa baa   \n4            word            sounds        1         baa baa       baa baa   \n5            word            sounds        1         baa baa       baa baa   \n...           ...               ...      ...             ...           ...   \n4963438      word  connecting_words      680            then          then   \n4963441      word  connecting_words      680            then          then   \n4963444      word  connecting_words      680            then          then   \n4963446      word  connecting_words      680            then          then   \n4963449      word  connecting_words      680            then          then   \n\n        uni_lemma  child_id  age                wordBankId     word  \n0         baa baa         1   28  651de3dbf3a9be0887dd1d86  baa baa  \n2         baa baa         3   26  651de3dbf3a9be0887dd1d86  baa baa  \n3         baa baa         4   27  651de3dbf3a9be0887dd1d86  baa baa  \n4         baa baa         5   19  651de3dbf3a9be0887dd1d86  baa baa  \n5         baa baa         6   30  651de3dbf3a9be0887dd1d86  baa baa  \n...           ...       ...  ...                       ...      ...  \n4963438      then     86601   29  651de3dbf3a9be0887dd202d     then  \n4963441      then     86604   30  651de3dbf3a9be0887dd202d     then  \n4963444      then     86607   25  651de3dbf3a9be0887dd202d     then  \n4963446      then     86609   29  651de3dbf3a9be0887dd202d     then  \n4963449      then     86612   29  651de3dbf3a9be0887dd202d     then  \n\n[2013822 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_id</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>child_id</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>1</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>3</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>4</td>\n      <td>27</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>5</td>\n      <td>19</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>1</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>6</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4963438</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86601</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963441</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86604</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963444</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86607</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963446</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86609</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n    <tr>\n      <th>4963449</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>680</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>86612</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>2013822 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df[\"item_kind\"].isin([\"combine\", \"word_endings\", \"word_endings_nouns\", \"word_endings_verbs\", \"how_use_words\"])\n",
    "df.drop(index=df[mask].index, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a4dda19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:40:08.038415900Z",
     "start_time": "2023-10-06T20:40:07.965813501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "item_kind\nword    2013822\nName: count, dtype: int64"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"item_kind\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7668d",
   "metadata": {},
   "source": [
    "That's still enough data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# Serialize the `df_words` because we'll use it when doing inference with the API\n",
    "mask = df_words[\"item_kind\"].isin([\"combine\", \"word_endings\", \"word_endings_nouns\", \"word_endings_verbs\", \"how_use_words\"])\n",
    "df_words.drop(index=df_words[mask].index, inplace=True)\n",
    "df_words[[\"word\", \"wordBankId\"]].to_parquet(\"words.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T20:40:37.847694914Z",
     "start_time": "2023-10-06T20:40:37.821695247Z"
    }
   },
   "id": "a7c7b29658e0db73"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "        item_kind          category item_definition   english_gloss  \\\nitem_id                                                               \n1            word            sounds         baa baa         baa baa   \n2            word            sounds       choo choo       choo choo   \n3            word            sounds  cockadoodledoo  cockadoodledoo   \n4            word            sounds            grrr            grrr   \n5            word            sounds            meow            meow   \n...           ...               ...             ...             ...   \n676          word  connecting_words         because         because   \n677          word  connecting_words             but             but   \n678          word  connecting_words              if              if   \n679          word  connecting_words              so              so   \n680          word  connecting_words            then            then   \n\n              uni_lemma                wordBankId            word  \nitem_id                                                            \n1               baa baa  651de3dbf3a9be0887dd1d86         baa baa  \n2             choo choo  651de3dbf3a9be0887dd1d87       choo choo  \n3        cockadoodledoo  651de3dbf3a9be0887dd1d88  cockadoodledoo  \n4                  grrr  651de3dbf3a9be0887dd1d89            grrr  \n5                  meow  651de3dbf3a9be0887dd1d8a            meow  \n...                 ...                       ...             ...  \n676             because  651de3dbf3a9be0887dd2029         because  \n677                 but  651de3dbf3a9be0887dd202a             but  \n678                  if  651de3dbf3a9be0887dd202b              if  \n679                  so  651de3dbf3a9be0887dd202c              so  \n680                then  651de3dbf3a9be0887dd202d            then  \n\n[653 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>wordBankId</th>\n      <th>word</th>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>baa baa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>choo choo</td>\n      <td>651de3dbf3a9be0887dd1d87</td>\n      <td>choo choo</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>cockadoodledoo</td>\n      <td>651de3dbf3a9be0887dd1d88</td>\n      <td>cockadoodledoo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>grrr</td>\n      <td>651de3dbf3a9be0887dd1d89</td>\n      <td>grrr</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>word</td>\n      <td>sounds</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>meow</td>\n      <td>651de3dbf3a9be0887dd1d8a</td>\n      <td>meow</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>676</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>because</td>\n      <td>because</td>\n      <td>because</td>\n      <td>651de3dbf3a9be0887dd2029</td>\n      <td>because</td>\n    </tr>\n    <tr>\n      <th>677</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>but</td>\n      <td>but</td>\n      <td>but</td>\n      <td>651de3dbf3a9be0887dd202a</td>\n      <td>but</td>\n    </tr>\n    <tr>\n      <th>678</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>if</td>\n      <td>if</td>\n      <td>if</td>\n      <td>651de3dbf3a9be0887dd202b</td>\n      <td>if</td>\n    </tr>\n    <tr>\n      <th>679</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>so</td>\n      <td>so</td>\n      <td>so</td>\n      <td>651de3dbf3a9be0887dd202c</td>\n      <td>so</td>\n    </tr>\n    <tr>\n      <th>680</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>then</td>\n      <td>then</td>\n      <td>then</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>then</td>\n    </tr>\n  </tbody>\n</table>\n<p>653 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T20:40:38.277496801Z",
     "start_time": "2023-10-06T20:40:38.268687157Z"
    }
   },
   "id": "5463bf0bcad5a6e"
  },
  {
   "cell_type": "markdown",
   "id": "94480002",
   "metadata": {},
   "source": [
    "# Prepare dataframe for LibRecommender\n",
    "\n",
    "From the Github readme:\n",
    "\n",
    "> JUST normal data format, each line represents a sample.\n",
    "> One thing is important, the model assumes that user, item, and label column index are 0, 1, and 2, respectively.\n",
    "> You may wish to change the column order if that's not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ece8a406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:09.885345631Z",
     "start_time": "2023-10-06T20:41:09.460646064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          user  item  label     word english_gloss  age  \\\n0            1     1    NaN  baa baa       baa baa   28   \n1            3     1    NaN  baa baa       baa baa   26   \n2            4     1    NaN  baa baa       baa baa   27   \n3            5     1    NaN  baa baa       baa baa   19   \n4            6     1    NaN  baa baa       baa baa   30   \n...        ...   ...    ...      ...           ...  ...   \n2013817  86601   680    NaN     then          then   29   \n2013818  86604   680    NaN     then          then   30   \n2013819  86607   680    NaN     then          then   25   \n2013820  86609   680    NaN     then          then   29   \n2013821  86612   680    NaN     then          then   29   \n\n                       wordBankId item_kind uni_lemma item_definition  \\\n0        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n1        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n2        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n3        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n4        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n...                           ...       ...       ...             ...   \n2013817  651de3dbf3a9be0887dd202d      word      then            then   \n2013818  651de3dbf3a9be0887dd202d      word      then            then   \n2013819  651de3dbf3a9be0887dd202d      word      then            then   \n2013820  651de3dbf3a9be0887dd202d      word      then            then   \n2013821  651de3dbf3a9be0887dd202d      word      then            then   \n\n                 category  \n0                  sounds  \n1                  sounds  \n2                  sounds  \n3                  sounds  \n4                  sounds  \n...                   ...  \n2013817  connecting_words  \n2013818  connecting_words  \n2013819  connecting_words  \n2013820  connecting_words  \n2013821  connecting_words  \n\n[2013822 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>item</th>\n      <th>label</th>\n      <th>word</th>\n      <th>english_gloss</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>item_kind</th>\n      <th>uni_lemma</th>\n      <th>item_definition</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>27</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>19</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2013817</th>\n      <td>86601</td>\n      <td>680</td>\n      <td>NaN</td>\n      <td>then</td>\n      <td>then</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013818</th>\n      <td>86604</td>\n      <td>680</td>\n      <td>NaN</td>\n      <td>then</td>\n      <td>then</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013819</th>\n      <td>86607</td>\n      <td>680</td>\n      <td>NaN</td>\n      <td>then</td>\n      <td>then</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013820</th>\n      <td>86609</td>\n      <td>680</td>\n      <td>NaN</td>\n      <td>then</td>\n      <td>then</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013821</th>\n      <td>86612</td>\n      <td>680</td>\n      <td>NaN</td>\n      <td>then</td>\n      <td>then</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n  </tbody>\n</table>\n<p>2013822 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.rename(columns={\"child_id\": \"user\", \"item_id\": \"item\"})\n",
    "data.insert(loc=2, column=\"label\", value=np.nan)\n",
    "\n",
    "other_columns = set(data.columns).difference([\"user\", \"item\", \"label\"])\n",
    "data = data[[\"user\", \"item\", \"label\"] + list(other_columns)]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e809f4a",
   "metadata": {},
   "source": [
    "## The value to use for the `label` column\n",
    "\n",
    "The column `label` denote how much \"interaction\" a children had with the word.\n",
    "\n",
    "In the classic recommendation example, where we're recommending movies for users, this is the **rating** that a user has given a movie.\n",
    "\n",
    "Unfortunately we don't have anything in our dataset that denotes this value, so we'll simply use the value `1.0`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4ac749b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:11.478371334Z",
     "start_time": "2023-10-06T20:41:11.459912791Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"label\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02d99425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:11.519540614Z",
     "start_time": "2023-10-06T20:41:11.471079577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2013822 entries, 0 to 2013821\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   user             int64  \n",
      " 1   item             int64  \n",
      " 2   label            float64\n",
      " 3   word             object \n",
      " 4   english_gloss    object \n",
      " 5   age              int64  \n",
      " 6   wordBankId       object \n",
      " 7   item_kind        object \n",
      " 8   uni_lemma        object \n",
      " 9   item_definition  object \n",
      " 10  category         object \n",
      "dtypes: float64(1), int64(3), object(7)\n",
      "memory usage: 169.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3984c744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:12.400169564Z",
     "start_time": "2023-10-06T20:41:12.281848142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          user  item  label     word english_gloss  age  \\\n0            1     1    1.0  baa baa       baa baa   28   \n1            3     1    1.0  baa baa       baa baa   26   \n2            4     1    1.0  baa baa       baa baa   27   \n3            5     1    1.0  baa baa       baa baa   19   \n4            6     1    1.0  baa baa       baa baa   30   \n...        ...   ...    ...      ...           ...  ...   \n2013818  86604   680    1.0     then          then   30   \n2013819  86607   680    1.0     then          then   25   \n2013820  86609   680    1.0     then          then   29   \n2013821  86612   680    1.0     then          then   29   \n2013822     -1    -1    0.0      NaN       unknown    0   \n\n                       wordBankId item_kind uni_lemma item_definition  \\\n0        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n1        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n2        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n3        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n4        651de3dbf3a9be0887dd1d86      word   baa baa         baa baa   \n...                           ...       ...       ...             ...   \n2013818  651de3dbf3a9be0887dd202d      word      then            then   \n2013819  651de3dbf3a9be0887dd202d      word      then            then   \n2013820  651de3dbf3a9be0887dd202d      word      then            then   \n2013821  651de3dbf3a9be0887dd202d      word      then            then   \n2013822                       NaN   unknown   unknown         unknown   \n\n                 category  \n0                  sounds  \n1                  sounds  \n2                  sounds  \n3                  sounds  \n4                  sounds  \n...                   ...  \n2013818  connecting_words  \n2013819  connecting_words  \n2013820  connecting_words  \n2013821  connecting_words  \n2013822           unknown  \n\n[2013823 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>item</th>\n      <th>label</th>\n      <th>word</th>\n      <th>english_gloss</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>item_kind</th>\n      <th>uni_lemma</th>\n      <th>item_definition</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>27</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>19</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd1d86</td>\n      <td>word</td>\n      <td>baa baa</td>\n      <td>baa baa</td>\n      <td>sounds</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2013818</th>\n      <td>86604</td>\n      <td>680</td>\n      <td>1.0</td>\n      <td>then</td>\n      <td>then</td>\n      <td>30</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013819</th>\n      <td>86607</td>\n      <td>680</td>\n      <td>1.0</td>\n      <td>then</td>\n      <td>then</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013820</th>\n      <td>86609</td>\n      <td>680</td>\n      <td>1.0</td>\n      <td>then</td>\n      <td>then</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013821</th>\n      <td>86612</td>\n      <td>680</td>\n      <td>1.0</td>\n      <td>then</td>\n      <td>then</td>\n      <td>29</td>\n      <td>651de3dbf3a9be0887dd202d</td>\n      <td>word</td>\n      <td>then</td>\n      <td>then</td>\n      <td>connecting_words</td>\n    </tr>\n    <tr>\n      <th>2013822</th>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>unknown</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>unknown</td>\n      <td>unknown</td>\n      <td>unknown</td>\n      <td>unknown</td>\n    </tr>\n  </tbody>\n</table>\n<p>2013823 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a \"fake\" user so that we can retrain the model later\n",
    "data = pd.concat([data, pd.DataFrame(\n",
    "    {\"user\": -1,\n",
    "     \"item\": -1,\n",
    "     \"label\": 0.0,\n",
    "     \"category\": \"unknown\",\n",
    "     \"item_kind\": \"unknown\",\n",
    "     \"uni_lemma\": \"unknown\",\n",
    "     \"item_definition\": \"unknown\",\n",
    "     \"english_gloss\": \"unknown\",\n",
    "     \"age\": 0,\n",
    "     },\n",
    "    index=[len(data)]\n",
    ")])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4dc3fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:14.334907657Z",
     "start_time": "2023-10-06T20:41:14.297784687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         user  item  label word english_gloss  age wordBankId item_kind  \\\n2013822    -1    -1    0.0  NaN       unknown    0        NaN   unknown   \n\n        uni_lemma item_definition category  \n2013822   unknown         unknown  unknown  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>item</th>\n      <th>label</th>\n      <th>word</th>\n      <th>english_gloss</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>item_kind</th>\n      <th>uni_lemma</th>\n      <th>item_definition</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013822</th>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>unknown</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>unknown</td>\n      <td>unknown</td>\n      <td>unknown</td>\n      <td>unknown</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"user\"] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532cff4",
   "metadata": {},
   "source": [
    "Following the [tutorial](https://librecommender.readthedocs.io/en/latest/tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30e9afd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:18.345187585Z",
     "start_time": "2023-10-06T20:41:16.554460880Z"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.data import random_split\n",
    "\n",
    "# split data into three folds for training, evaluating and testing\n",
    "train_data, eval_data, test_data = random_split(data, multi_ratios=[0.8, 0.1, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "968bee0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:18.401628700Z",
     "start_time": "2023-10-06T20:41:18.359655913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          user  item  label       word english_gloss  age  \\\n628112     824   176    1.0    slipper       slipper   28   \n872975   62818   246    1.0      purse         purse   24   \n1398736   3187   422    1.0      drive         drive   20   \n448204    3582   116    1.0  hamburger     hamburger   28   \n1192385   2889   359    1.0     friend        friend   25   \n...        ...   ...    ...        ...           ...  ...   \n182221   84627    42    1.0        owl           owl   20   \n1230595   3841   374    1.0    teacher       teacher   28   \n412478    3717   104    1.0     cookie        cookie   28   \n844717    1343   237    1.0        mop           mop   26   \n1610475  86024   496    1.0       wake          wake   24   \n\n                       wordBankId item_kind     uni_lemma item_definition  \\\n628112   651de3dbf3a9be0887dd1e35      word       slipper         slipper   \n872975   651de3dbf3a9be0887dd1e7b      word         purse           purse   \n1398736  651de3dbf3a9be0887dd1f2b      word         drive           drive   \n448204   651de3dbf3a9be0887dd1df9      word     hamburger       hamburger   \n1192385  651de3dbf3a9be0887dd1eec      word        friend          friend   \n...                           ...       ...           ...             ...   \n182221   651de3dbf3a9be0887dd1daf      word           owl             owl   \n1230595  651de3dbf3a9be0887dd1efb      word       teacher         teacher   \n412478   651de3dbf3a9be0887dd1ded      word        cookie          cookie   \n844717   651de3dbf3a9be0887dd1e72      word  mop (object)             mop   \n1610475  651de3dbf3a9be0887dd1f75      word          wake            wake   \n\n             category  \n628112       clothing  \n872975      household  \n1398736  action_words  \n448204     food_drink  \n1192385        people  \n...               ...  \n182221        animals  \n1230595        people  \n412478     food_drink  \n844717      household  \n1610475  action_words  \n\n[1611057 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>item</th>\n      <th>label</th>\n      <th>word</th>\n      <th>english_gloss</th>\n      <th>age</th>\n      <th>wordBankId</th>\n      <th>item_kind</th>\n      <th>uni_lemma</th>\n      <th>item_definition</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>628112</th>\n      <td>824</td>\n      <td>176</td>\n      <td>1.0</td>\n      <td>slipper</td>\n      <td>slipper</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1e35</td>\n      <td>word</td>\n      <td>slipper</td>\n      <td>slipper</td>\n      <td>clothing</td>\n    </tr>\n    <tr>\n      <th>872975</th>\n      <td>62818</td>\n      <td>246</td>\n      <td>1.0</td>\n      <td>purse</td>\n      <td>purse</td>\n      <td>24</td>\n      <td>651de3dbf3a9be0887dd1e7b</td>\n      <td>word</td>\n      <td>purse</td>\n      <td>purse</td>\n      <td>household</td>\n    </tr>\n    <tr>\n      <th>1398736</th>\n      <td>3187</td>\n      <td>422</td>\n      <td>1.0</td>\n      <td>drive</td>\n      <td>drive</td>\n      <td>20</td>\n      <td>651de3dbf3a9be0887dd1f2b</td>\n      <td>word</td>\n      <td>drive</td>\n      <td>drive</td>\n      <td>action_words</td>\n    </tr>\n    <tr>\n      <th>448204</th>\n      <td>3582</td>\n      <td>116</td>\n      <td>1.0</td>\n      <td>hamburger</td>\n      <td>hamburger</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1df9</td>\n      <td>word</td>\n      <td>hamburger</td>\n      <td>hamburger</td>\n      <td>food_drink</td>\n    </tr>\n    <tr>\n      <th>1192385</th>\n      <td>2889</td>\n      <td>359</td>\n      <td>1.0</td>\n      <td>friend</td>\n      <td>friend</td>\n      <td>25</td>\n      <td>651de3dbf3a9be0887dd1eec</td>\n      <td>word</td>\n      <td>friend</td>\n      <td>friend</td>\n      <td>people</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>182221</th>\n      <td>84627</td>\n      <td>42</td>\n      <td>1.0</td>\n      <td>owl</td>\n      <td>owl</td>\n      <td>20</td>\n      <td>651de3dbf3a9be0887dd1daf</td>\n      <td>word</td>\n      <td>owl</td>\n      <td>owl</td>\n      <td>animals</td>\n    </tr>\n    <tr>\n      <th>1230595</th>\n      <td>3841</td>\n      <td>374</td>\n      <td>1.0</td>\n      <td>teacher</td>\n      <td>teacher</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1efb</td>\n      <td>word</td>\n      <td>teacher</td>\n      <td>teacher</td>\n      <td>people</td>\n    </tr>\n    <tr>\n      <th>412478</th>\n      <td>3717</td>\n      <td>104</td>\n      <td>1.0</td>\n      <td>cookie</td>\n      <td>cookie</td>\n      <td>28</td>\n      <td>651de3dbf3a9be0887dd1ded</td>\n      <td>word</td>\n      <td>cookie</td>\n      <td>cookie</td>\n      <td>food_drink</td>\n    </tr>\n    <tr>\n      <th>844717</th>\n      <td>1343</td>\n      <td>237</td>\n      <td>1.0</td>\n      <td>mop</td>\n      <td>mop</td>\n      <td>26</td>\n      <td>651de3dbf3a9be0887dd1e72</td>\n      <td>word</td>\n      <td>mop (object)</td>\n      <td>mop</td>\n      <td>household</td>\n    </tr>\n    <tr>\n      <th>1610475</th>\n      <td>86024</td>\n      <td>496</td>\n      <td>1.0</td>\n      <td>wake</td>\n      <td>wake</td>\n      <td>24</td>\n      <td>651de3dbf3a9be0887dd1f75</td>\n      <td>word</td>\n      <td>wake</td>\n      <td>wake</td>\n      <td>action_words</td>\n    </tr>\n  </tbody>\n</table>\n<p>1611057 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40c23cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:19.758686284Z",
     "start_time": "2023-10-06T20:41:19.718341551Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, data_info, model_name: str):\n",
    "    model_type_name = type(model).__name__\n",
    "    Path(f\"models/{model_type_name}/data-info/\").mkdir(parents=True, exist_ok=True)\n",
    "    data_info.save(f\"models/{model_type_name}/data-info/\", model_name=model_name)\n",
    "    model.save(f\"models/{model_type_name}-retrain/weights/\", model_name=model_name, manual=True, inference_only=False)\n",
    "    model.save(f\"models/{model_type_name}-inference/weights/\", model_name=model_name, inference_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6ee31",
   "metadata": {},
   "source": [
    "# Using a \"Pure\" model\n",
    "\n",
    "We'll use the [LightGCN](https://librecommender.readthedocs.io/en/latest/api/algorithms/lightgcn.html) model because it is what we have from the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba3792",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The next cells train a LightGCN model.\n",
    "\n",
    "Skip to the next section (\"inference\") if you don't want to wait for the training of the model. It takes ~ 10 minutes to train.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed8b125e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:22.069718722Z",
     "start_time": "2023-10-06T20:41:22.040434063Z"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.algorithms import LightGCN  # pure data, algorithm LightGCN\n",
    "from libreco.data import DatasetPure\n",
    "from libreco.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88bb2abd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:29.556653421Z",
     "start_time": "2023-10-06T20:41:24.075839155Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_pure, data_info_pure = DatasetPure.build_trainset(train_data)\n",
    "eval_data_pure = DatasetPure.build_evalset(eval_data)\n",
    "test_data_pure = DatasetPure.build_testset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22633f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:29.601833254Z",
     "start_time": "2023-10-06T20:41:29.557645262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "n_users: 6357, n_items: 654, data density: 38.7508 %"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info_pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6af1a931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:41:32.716520354Z",
     "start_time": "2023-10-06T20:41:32.677263260Z"
    }
   },
   "outputs": [],
   "source": [
    "lightgcn = LightGCN(\n",
    "    task=\"ranking\",\n",
    "    data_info=data_info_pure,\n",
    "    loss_type=\"bpr\",\n",
    "    embed_size=16,\n",
    "    n_epochs=3,\n",
    "    lr=1e-3,\n",
    "    batch_size=2048,\n",
    "    num_neg=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# TODO: Experiment with the hyperparameters to make metrics better. \n",
    "# lightgcn = LightGCN(\n",
    "#     task=\"ranking\",\n",
    "#     data_info=data_info,\n",
    "#     loss_type=\"bpr\",\n",
    "#     embed_size=16,\n",
    "#     n_epochs=3,\n",
    "#     lr=1e-1,\n",
    "#     batch_size=2048,\n",
    "#     num_neg=1,\n",
    "#     device=\"cuda\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "276576a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:55:08.618458277Z",
     "start_time": "2023-10-06T20:41:33.670850193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-10-06 22:41:33\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 787/787 [04:24<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 264.969s\n",
      "\t \u001B[32mtrain_loss: 0.6612\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 50/50 [00:00<00:00, 720.44it/s]\n",
      "/home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 516/516 [00:00<00:00, 885.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.7551\n",
      "\t eval roc_auc: 0.6247\n",
      "\t eval precision@10: 0.3274\n",
      "\t eval recall@10: 0.1585\n",
      "\t eval ndcg@10: 0.6465\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 787/787 [04:32<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 272.140s\n",
      "\t \u001B[32mtrain_loss: 0.6295\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 50/50 [00:00<00:00, 547.37it/s]\n",
      "/home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 516/516 [00:00<00:00, 853.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.7390\n",
      "\t eval roc_auc: 0.6182\n",
      "\t eval precision@10: 0.3304\n",
      "\t eval recall@10: 0.1587\n",
      "\t eval ndcg@10: 0.6474\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 787/787 [04:27<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 267.450s\n",
      "\t \u001B[32mtrain_loss: 0.6235\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 50/50 [00:00<00:00, 640.16it/s]\n",
      "/home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 516/516 [00:00<00:00, 841.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.7347\n",
      "\t eval roc_auc: 0.6159\n",
      "\t eval precision@10: 0.3320\n",
      "\t eval recall@10: 0.1588\n",
      "\t eval ndcg@10: 0.6474\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 50/50 [00:00<00:00, 722.43it/s]\n",
      "/home/gustavo/PycharmProjects/tici-turing/ss23-talk-a-palooza/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 516/516 [00:00<00:00, 888.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_result:  {'loss': 0.7346561560966327, 'roc_auc': 0.6154107970435512, 'precision': 0.32975420439844766, 'recall': 0.1604368666406779, 'ndcg': 0.650183924354682}\n"
     ]
    }
   ],
   "source": [
    "# monitor metrics on eval_data during training\n",
    "lightgcn.fit(\n",
    "    train_data_pure,\n",
    "    neg_sampling=True,  # sample negative items for train and eval data\n",
    "    verbose=2,\n",
    "    eval_data=eval_data_pure,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "# do final evaluation on test data\n",
    "print(\n",
    "    \"evaluate_result: \",\n",
    "    evaluate(\n",
    "        model=lightgcn,\n",
    "        data=test_data_pure,\n",
    "        neg_sampling=True,  # sample negative items for test data\n",
    "        metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b38d83",
   "metadata": {},
   "source": [
    "Save the model so we don't need to retrain. Also save an \"inference-only\" version in case someone exploring the notebook wants to only look at results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dffa49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:56:18.656655495Z",
     "start_time": "2023-10-06T20:56:16.965654042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file folder models/LightGCN-retrain/weights/ doesn't exists, creating a new one...\n",
      "file folder models/LightGCN-inference/weights/ doesn't exists, creating a new one...\n"
     ]
    }
   ],
   "source": [
    "save_model(lightgcn, data_info_pure, \"lightgcn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e8499",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Let's load the model and look at results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf1e5606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T20:12:19.587592095Z",
     "start_time": "2023-09-28T20:12:19.545534596Z"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.data import DataInfo\n",
    "from libreco.algorithms import LightGCN \n",
    "\n",
    "try:\n",
    "    lightgcn\n",
    "except NameError:\n",
    "    data_info = DataInfo.load(\"models/LightGCN/data-info/\", model_name=\"epochs=3\")\n",
    "    lightgcn = LightGCN.load(\"models/LightGCN-inference/weights/\", model_name=\"epochs=3\", data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5929f2b",
   "metadata": {},
   "source": [
    "## Looking at the model results\n",
    "\n",
    "First let's look at the results for children that are **already** in the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3b47476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:57:31.469089941Z",
     "start_time": "2023-10-06T20:57:31.304770411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(BoundedIntText(value=0, description='Child:', max=6359), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87d325bcb17342f0a82575c1b2142e71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets\n",
    "from IPython.display import display\n",
    "from typing import Dict\n",
    "\n",
    "df_children = df[[\"child_id\", \"age\"]].drop_duplicates(\"child_id\")\n",
    "df_children.set_index(\"child_id\", inplace=True)\n",
    "\n",
    "@interact(x=widgets.BoundedIntText(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(df_children),\n",
    "    step=1,\n",
    "    description='Child:',\n",
    "    disabled=False\n",
    "))\n",
    "def foo(x):\n",
    "    child_id = df_children.index[x]\n",
    "    print(f\"-> Child id is: {child_id}\")\n",
    "    print(f\"-> Child age: {df_children.loc[child_id].age}\")\n",
    "    words = set(df[df[\"child_id\"] == child_id][\"word\"].unique())\n",
    "    print(f\"-> Words spoken by this child: {len(words)}\")\n",
    "    display(\" | \".join(sorted(words)))\n",
    "    \n",
    "    recommendation: Dict[int, np.ndarray] = lightgcn.recommend_user(user=child_id, n_rec=100)\n",
    "    word_ids = recommendation[child_id]\n",
    "    \n",
    "    scores = lightgcn.predict(user=child_id, item=word_ids)\n",
    "    \n",
    "    display(\"-> Recommended words:\")\n",
    "    df_result = df_words.loc[word_ids]\n",
    "    \n",
    "    # Add column to know if the child already speaks such word (from the original dataset) \n",
    "    df_result[\"speaks?\"] = df_result[\"word\"].apply(lambda w: w in words)\n",
    "    df_result[\"score\"] = scores\n",
    "    display(df_result[df_result[\"speaks?\"] == False].iloc[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31580099",
   "metadata": {},
   "source": [
    "## Return words given an arbitrary set of words from the original dataset\n",
    "\n",
    "In this part we retrain the model for every request. Afterwards we ask for a recommendation of the child.\n",
    "\n",
    "We use the \"fake\" `child_id == -1` as explained in the Introduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "546bd687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:57:41.542196429Z",
     "start_time": "2023-10-06T20:57:41.516798902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['bring',\n 'beans',\n 'not',\n 'beach',\n 'necklace',\n 'eat',\n 'flag',\n 'pool',\n 'lamb',\n 'cup']"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample some random words, just for the sake of trying to make the model work\n",
    "words = df_words.sample(n=10)[\"word\"].tolist()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ce683e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T20:58:04.801395234Z",
     "start_time": "2023-10-06T20:58:00.881255059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-10-06 22:58:04\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 0.332s\n",
      "\t \u001B[32mtrain_loss: 0.8323\u001B[0m\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "        item_kind          category item_definition english_gloss uni_lemma  \\\nitem_id                                                                       \n643          word       quantifiers            each          each      each   \n329          word            places         country       country   country   \n674          word     helping_verbs           would         would     would   \n603          word          pronouns        yourself      yourself  2SG.REFL   \n678          word  connecting_words              if            if        if   \n658          word     helping_verbs           could         could     could   \n\n                       wordBankId      word     score  speaks?  \nitem_id                                                         \n643      651de3dbf3a9be0887dd2008      each  0.999580    False  \n329      651de3dbf3a9be0887dd1ece   country  0.999574    False  \n674      651de3dbf3a9be0887dd2027     would  0.999552    False  \n603      651de3dbf3a9be0887dd1fe0  yourself  0.999191    False  \n678      651de3dbf3a9be0887dd202b        if  0.999153    False  \n658      651de3dbf3a9be0887dd2017     could  0.999124    False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_kind</th>\n      <th>category</th>\n      <th>item_definition</th>\n      <th>english_gloss</th>\n      <th>uni_lemma</th>\n      <th>wordBankId</th>\n      <th>word</th>\n      <th>score</th>\n      <th>speaks?</th>\n    </tr>\n    <tr>\n      <th>item_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>643</th>\n      <td>word</td>\n      <td>quantifiers</td>\n      <td>each</td>\n      <td>each</td>\n      <td>each</td>\n      <td>651de3dbf3a9be0887dd2008</td>\n      <td>each</td>\n      <td>0.999580</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>329</th>\n      <td>word</td>\n      <td>places</td>\n      <td>country</td>\n      <td>country</td>\n      <td>country</td>\n      <td>651de3dbf3a9be0887dd1ece</td>\n      <td>country</td>\n      <td>0.999574</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>674</th>\n      <td>word</td>\n      <td>helping_verbs</td>\n      <td>would</td>\n      <td>would</td>\n      <td>would</td>\n      <td>651de3dbf3a9be0887dd2027</td>\n      <td>would</td>\n      <td>0.999552</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>603</th>\n      <td>word</td>\n      <td>pronouns</td>\n      <td>yourself</td>\n      <td>yourself</td>\n      <td>2SG.REFL</td>\n      <td>651de3dbf3a9be0887dd1fe0</td>\n      <td>yourself</td>\n      <td>0.999191</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>678</th>\n      <td>word</td>\n      <td>connecting_words</td>\n      <td>if</td>\n      <td>if</td>\n      <td>if</td>\n      <td>651de3dbf3a9be0887dd202b</td>\n      <td>if</td>\n      <td>0.999153</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>658</th>\n      <td>word</td>\n      <td>helping_verbs</td>\n      <td>could</td>\n      <td>could</td>\n      <td>could</td>\n      <td>651de3dbf3a9be0887dd2017</td>\n      <td>could</td>\n      <td>0.999124</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "from libreco.data import DataInfo\n",
    "\n",
    "def predict(words: List[str]):\n",
    "    child_id = -1\n",
    "    # Train the model with the words spoken by the new child\n",
    "    words_ids = df_words[df_words[\"word\"].isin(words)].index.tolist()\n",
    "    df_train = pd.DataFrame({\"user\": child_id, \"item\": words_ids, \"label\": 1.0})\n",
    "    \n",
    "    old_data_info = DataInfo.load(\"models/LightGCN/data-info\", model_name=\"lightgcn\")\n",
    "    data, data_info = DatasetPure.merge_trainset(df_train, old_data_info)\n",
    "    model = LightGCN(\n",
    "        task=\"ranking\",\n",
    "        data_info=data_info,\n",
    "        loss_type=\"bpr\",\n",
    "        embed_size=16,\n",
    "        n_epochs=1,\n",
    "        lr=1e-3,\n",
    "        batch_size=2048,\n",
    "        num_neg=1,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    model.rebuild_model(\"models/LightGCN-retrain/weights\", model_name=\"lightgcn\")\n",
    "    \n",
    "    model.fit(\n",
    "        data,\n",
    "        neg_sampling=True,  # sample negative items for train and eval data\n",
    "        verbose=2,\n",
    "    )\n",
    "    \n",
    "    # Predict the words for this child\n",
    "    recommendation: Dict[int, np.ndarray] = lightgcn.recommend_user(user=child_id, n_rec=100)\n",
    "    predicted_word_ids = recommendation[child_id]\n",
    "    scores = lightgcn.predict(user=child_id, item=predicted_word_ids)\n",
    "    \n",
    "    df_result = df_words.loc[predicted_word_ids]\n",
    "    df_result[\"score\"] = scores\n",
    "    df_result[\"speaks?\"] = df_result[\"word\"].apply(lambda w: w in words)\n",
    "    display(df_result[~df_result[\"speaks?\"]].iloc[:6])\n",
    "    \n",
    "predict(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5c01",
   "metadata": {},
   "source": [
    "# \"Feat\" algorithm\n",
    "\n",
    "We'll use the Wide & Deep model.\n",
    "\n",
    "This model is based on the paper [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf).\n",
    "\n",
    "The LibRecommender [documentation](https://librecommender.readthedocs.io/en/latest/api/algorithms/wide_deep.html) contains more information on how to use this class.\n",
    "\n",
    "There is also a [tutorial](https://librecommender.readthedocs.io/en/latest/tutorial.html) at the LibRecommender website showing how to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the data split into train/test/eval. We'll just build the model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e74c582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T20:51:14.286804008Z",
     "start_time": "2023-09-28T20:51:14.257221770Z"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.algorithms import WideDeep\n",
    "from libreco.data import DatasetFeat\n",
    "from libreco.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6271c",
   "metadata": {},
   "source": [
    "From the tutorial:\n",
    "> \n",
    "> In LibRecommender we use `sparse_col` to represent categorical features and `dense_col` to represent numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6398f5d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T20:56:03.632017421Z",
     "start_time": "2023-09-28T20:55:58.441877192Z"
    }
   },
   "outputs": [],
   "source": [
    "sparse_col = [\n",
    "    \"item_kind\",\n",
    "    # \"category\",  # TODO: Fill with \"unknown\"?\n",
    "    # Maybe we could use \"race\" here too.\n",
    "]\n",
    "dense_col = [\"age\"]\n",
    "user_col = [\"age\"]\n",
    "item_col = [\n",
    "    \"item_kind\",\n",
    "    # \"category\"\n",
    "]\n",
    "\n",
    "train_data_feat, data_info_feat = DatasetFeat.build_trainset(train_data, user_col, item_col, sparse_col, dense_col)\n",
    "eval_data_feat = DatasetFeat.build_evalset(eval_data)\n",
    "test_data_feat = DatasetFeat.build_testset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d68d94a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:09:30.736298892Z",
     "start_time": "2023-09-28T21:09:30.696002635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_users: 6365, n_items: 743, data density: 39.7250 %"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46d89efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:15:08.981070511Z",
     "start_time": "2023-09-28T21:12:29.886492101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-09-28 23:12:29\u001B[0m\n",
      "total params: \u001B[33m140,195\u001B[0m | embedding params: \u001B[33m121,215\u001B[0m | network params: \u001B[33m18,980\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/layers/dense.py:31: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  net = tf.layers.batch_normalization(net, training=is_training)\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/layers/dense.py:39: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  net = tf.layers.batch_normalization(net, training=is_training)\n",
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 161.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 11.386s\n",
      "\t \u001B[32mtrain_loss: 0.6996\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 159.48it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 167.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6421\n",
      "\t eval roc_auc: 0.6582\n",
      "\t eval precision@10: 0.3597\n",
      "\t eval recall@10: 0.1640\n",
      "\t eval ndcg@10: 0.6775\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 161.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 11.354s\n",
      "\t \u001B[32mtrain_loss: 0.6368\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 179.65it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 167.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6386\n",
      "\t eval roc_auc: 0.6639\n",
      "\t eval precision@10: 0.3831\n",
      "\t eval recall@10: 0.1846\n",
      "\t eval ndcg@10: 0.6939\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 164.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 11.130s\n",
      "\t \u001B[32mtrain_loss: 0.6343\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 181.49it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 168.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6384\n",
      "\t eval roc_auc: 0.6650\n",
      "\t eval precision@10: 0.3796\n",
      "\t eval recall@10: 0.1825\n",
      "\t eval ndcg@10: 0.6919\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 163.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 elapsed: 11.241s\n",
      "\t \u001B[32mtrain_loss: 0.6331\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 180.31it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 166.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6381\n",
      "\t eval roc_auc: 0.6660\n",
      "\t eval precision@10: 0.3970\n",
      "\t eval recall@10: 0.1888\n",
      "\t eval ndcg@10: 0.7058\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 164.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 elapsed: 11.127s\n",
      "\t \u001B[32mtrain_loss: 0.6316\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 181.73it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 166.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6374\n",
      "\t eval roc_auc: 0.6668\n",
      "\t eval precision@10: 0.4079\n",
      "\t eval recall@10: 0.1946\n",
      "\t eval ndcg@10: 0.7070\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 165.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 elapsed: 11.118s\n",
      "\t \u001B[32mtrain_loss: 0.6298\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 175.72it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 169.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6365\n",
      "\t eval roc_auc: 0.6675\n",
      "\t eval precision@10: 0.4095\n",
      "\t eval recall@10: 0.1951\n",
      "\t eval ndcg@10: 0.7083\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 164.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 elapsed: 11.182s\n",
      "\t \u001B[32mtrain_loss: 0.6283\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 179.98it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 167.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6357\n",
      "\t eval roc_auc: 0.6677\n",
      "\t eval precision@10: 0.3991\n",
      "\t eval recall@10: 0.1925\n",
      "\t eval ndcg@10: 0.7057\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 165.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 elapsed: 11.119s\n",
      "\t \u001B[32mtrain_loss: 0.627\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 179.11it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 167.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6358\n",
      "\t eval roc_auc: 0.6692\n",
      "\t eval precision@10: 0.4095\n",
      "\t eval recall@10: 0.1966\n",
      "\t eval ndcg@10: 0.7111\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 164.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 elapsed: 11.139s\n",
      "\t \u001B[32mtrain_loss: 0.6257\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 180.01it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 167.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6373\n",
      "\t eval roc_auc: 0.6684\n",
      "\t eval precision@10: 0.4071\n",
      "\t eval recall@10: 0.1966\n",
      "\t eval ndcg@10: 0.7096\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:11<00:00, 164.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 elapsed: 11.138s\n",
      "\t \u001B[32mtrain_loss: 0.6248\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 156.71it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:03<00:00, 163.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6364\n",
      "\t eval roc_auc: 0.6688\n",
      "\t eval precision@10: 0.3992\n",
      "\t eval recall@10: 0.1923\n",
      "\t eval ndcg@10: 0.7015\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Need to call this otherwise this cell can only run once\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "wide_deep = WideDeep(\n",
    "    task=\"ranking\",\n",
    "    data_info=data_info_feat,\n",
    "    embed_size=16,\n",
    "    n_epochs=10,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    lr={\"wide\": 0.05, \"deep\": 7e-4},\n",
    "    batch_size=2048,\n",
    "    use_bn=True,\n",
    "    hidden_units=(128, 64, 32),\n",
    ")\n",
    "\n",
    "wide_deep.fit(\n",
    "    train_data_feat,\n",
    "    neg_sampling=True,  # perform negative sampling on training and eval data\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    "    eval_data=eval_data_feat,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d859e122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:22:25.143070163Z",
     "start_time": "2023-09-28T21:22:23.162451733Z"
    }
   },
   "outputs": [],
   "source": [
    "save_model(wide_deep, data_info_feat, \"epochs=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "51b3a607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:30:19.668574156Z",
     "start_time": "2023-09-28T21:27:10.114543685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001B[35m2023-09-28 23:27:10\u001B[0m\n",
      "total params: \u001B[33m134,609\u001B[0m | embedding params: \u001B[33m114,097\u001B[0m | network params: \u001B[33m20,512\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:58<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 58.924s\n",
      "\t \u001B[32mtrain_loss: 0.6743\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 251.49it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:02<00:00, 257.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6588\n",
      "\t eval balanced_accuracy: 0.6019\n",
      "\t eval roc_auc: 0.6402\n",
      "\t eval pr_auc: 0.6054\n",
      "\t eval precision@10: 0.3354\n",
      "\t eval recall@10: 0.1352\n",
      "\t eval map@10: 0.5019\n",
      "\t eval ndcg@10: 0.6419\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:58<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 58.754s\n",
      "\t \u001B[32mtrain_loss: 0.656\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 268.41it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:02<00:00, 253.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6492\n",
      "\t eval balanced_accuracy: 0.6019\n",
      "\t eval roc_auc: 0.6512\n",
      "\t eval pr_auc: 0.6341\n",
      "\t eval precision@10: 0.3502\n",
      "\t eval recall@10: 0.1569\n",
      "\t eval map@10: 0.5203\n",
      "\t eval ndcg@10: 0.6616\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1835/1835 [00:58<00:00, 31.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 58.694s\n",
      "\t \u001B[32mtrain_loss: 0.6471\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████| 58/58 [00:00<00:00, 270.84it/s]\n",
      "/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2845: FutureWarning: Setting the eps parameter is deprecated and will be removed in 1.5. Instead eps will always havea default value of `np.finfo(y_pred.dtype).eps`.\n",
      "  warnings.warn(\n",
      "eval_listwise: 100%|██████████| 572/572 [00:02<00:00, 257.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.6448\n",
      "\t eval balanced_accuracy: 0.6003\n",
      "\t eval roc_auc: 0.6535\n",
      "\t eval pr_auc: 0.6398\n",
      "\t eval precision@10: 0.3586\n",
      "\t eval recall@10: 0.1575\n",
      "\t eval map@10: 0.5167\n",
      "\t eval ndcg@10: 0.6532\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from libreco.algorithms import (\n",
    "    DIN,\n",
    "    FM,\n",
    "    AutoInt,\n",
    "    DeepFM,\n",
    "    GraphSage,\n",
    "    GraphSageDGL,\n",
    "    PinSage,\n",
    "    PinSageDGL,\n",
    "    TwoTower,\n",
    "    WideDeep,\n",
    "    YouTubeRanking,\n",
    "    YouTubeRetrieval,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    \"loss\",\n",
    "    \"balanced_accuracy\",\n",
    "    \"roc_auc\",\n",
    "    \"pr_auc\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"map\",\n",
    "    \"ndcg\",\n",
    "]\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "ytb_ranking = YouTubeRanking(\n",
    "    \"ranking\",\n",
    "    data_info_feat,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    embed_size=16,\n",
    "    n_epochs=3,\n",
    "    lr=1e-4,\n",
    "    lr_decay=False,\n",
    "    reg=None,\n",
    "    batch_size=2048,\n",
    "    num_neg=1,\n",
    "    use_bn=False,\n",
    "    dropout_rate=None,\n",
    "    hidden_units=(128, 64, 32),\n",
    "    tf_sess_config=None,\n",
    ")\n",
    "ytb_ranking.fit(\n",
    "    train_data_feat,\n",
    "    neg_sampling=True,\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    "    eval_data=eval_data_feat,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b409913b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:32:32.471699748Z",
     "start_time": "2023-09-28T21:32:32.209541383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Child id is: 6\n",
      "-> Child age: 30\n",
      "-> Words spoken by this child: 569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I | I fall down / I fell down | I like read stories / I like to read stories | I make tower / I making tower | I no do it / I can't do it | I sing song / I sing song for you | I want that / I want that one you got | TV | a | a lot | airplane | all | alligator | am | and | animal | ant | any | apple | applesauce | are | arm | around | asleep | ate | aunt | awake | baa baa | baby | baby blanket / baby's blanket | baby crying / baby crying cuz she's sad | baby crying / baby is crying | baby want eat / baby want to eat | back | backyard | bad | ball | balloon | banana | basket | bat | bath | bathroom | bathtub | beach | beans | bear | because | bed | bedroom | bee | behind | belly button | belt | bicycle | big | bird | bite | black | blanket | blew | block | blow | blue | boat | book | boots | bottle | bought | bowl | box | boy | bread | break | broke | broken | broom | brother | brush | bubbles | bucket | bug | bunny | bus | but | butter | butterfly | buttocks/bottom* | button | buy | by | bye | cake | call (on phone) | camera | can (auxiliary) | can (object) | candy | car | careful | carrots | carry | cat | catch | cereal | chair | chalk | chase | cheek | cheerios | cheese | chicken (animal) | chicken (food) | child's own name | chin | chocolate | choo choo | circus | clap | clean (action) | clean (description) | climb | clock | close | closet | clown | coat | cockadoodledoo | coffee hot / that coffee hot | coke | cold | comb | cook | cookie | cookie mommy / cookie for mommy | couch | cow | cowboy | cracker | crayon | crib | cry | cup | cut | cute | daddy car / daddy's car | daddy pick me up / daddy picked me up | daddy* | dance | deer | diaper | did/did ya | dinner | dirty | do | doctor | dog | doggie kiss me / doggie kissed me | doggie table / doggie on table | doll | don't | don't read book / don't want you read that book | donkey | donut | door | down | drank | draw | dress (object) | drink (action) | drink (beverage) | drive | drop | dry (action) | dry (description) | duck | ear | eat | egg | elephant | empty | eye | face | fall | fast | feed | feet | fell | find | fine | finger | first | fish (animal) | fish (food) | fit | fix | flower | food | foot | for | fork | french fries | friend | frog | game | garbage | gas station | get | giraffe | girl | give me five! | glasses | glue | go | go bye-bye / wanna go bye-bye | go potty | gonna get you! | gonna/going to | good | goose | got | gotta/got to | grandma* | grandpa* | grapes | grass | green | grrr | gum | had | hair | hamburger | hammer | hand | happy | hard | hat | he | head | heard | heavy | held | helicopter | hello | help | hen | her | here | hers | hi | hide | high | hit | home | horse | hose | hot | house | hug | hungry | hurry | hurt | ice | ice cream | inside/in | is | it | jacket | jeans | juice | jump | keys | kick | kiss | kitchen | kitty | kitty go away / kitty went away | kitty sleep / kitty sleeping | knife | knock | lady | lamb | lamp | later | leg | lemme/let me | lick | light | like | lion | lips | listen | little (description) | living room | look | lookit / lookit what I got | lookit me / lookit me dancing | lost | loud | love | lunch | mad | made | mailman | make | man | me | meat | medicine | meow | milk | mine | mommy* | money | monkey | moo | moon | moose | more | more cookie / more cookies | motorcycle | mouse | mouth | movie | nail | nap | napkin | necklace | nice | night | night night | no | no wash dolly / don't wash dolly | noisy | none | noodles | nose | not | nuts | of | off | on | open | orange (description) | orange (food) | other | ouch | out | outside | oven | over | owie/boo boo | owl | pancake | pants | paper | park | party | pattycake | peanut butter | peekaboo | pen | pencil | penguin | penis* | penny | people | pet's name | pickle | picture | pig | pillow | pizza | plant | plate | play | play dough | please | pony | popcorn | popsicle | potato | potato chip | potty | present | pretend | pretty | pretzel | pudding | pumpkin | puppy | purse | puzzle | quack quack | quiet | radio | raisin | ran | read | read me story Mommy / read me a story Mommy | red | refrigerator | ride | rock | rocking chair | room | rooster | run | sandwich | sat | sauce | saw | say | scared | school | scissors | see | share | she | sheep | shh/shush/hush | shirt | shoe | shopping | shorts | shovel | show | shower | sidewalk | sing | sink | sister | sit | sky | sleep | sleepy | slide (action) | slide (object) | slipper | snack | snow | snowman | so big! | soap | sock | soda/pop | soft | some | soup | spaghetti | splash | spoon | sprinkler | squirrel | stairs | star | stick | sticky | stone | stop | store | story | stove | strawberry | street | stroller | stuck | sun | sweep | swim | swing (action) | swing (object) | table | talk | tape | taste | teacher | tear | teddybear | teeth | telephone | thank you | that | that my truck / that's my truck | the | their | them | there a kitty / there's a kitty | these | these my tooth / these my teeth | this | this little piggy | throw | tickle | tiger | time | tired | to | toast | today | toe | tongue | too | took | tooth | toothbrush | touch | towel | toy (object) | tractor | train | trash | tree | tricycle | truck | tummy | tuna | turkey | turn around | turn on light / turn on light so I can see | turtle | two foot / two feet | two shoe / two shoes | uh oh | uncle | under | underpants | up | vacuum | vagina* | wait | walk | walker | wanna/want to | want cookies / want cookies and milk | want more juice / want juice in there | wash | watch (action) | watch (object) | water (beverage) | water (not beverage) | we | we made this / me and Paul made this | went | wet | what | when | where | where mommy go / where did mommy go | where's my dolly / where's my dolly name Sam | who | will | wind | window | with | wolf | woof woof | work (action) | write | yard | yes | yogurt | you | you fix it / can you fix it | yucky | yum yum | zebra | zipper | zoo\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'concat' defined at (most recent call last):\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n      app.start()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_20088/4289332625.py\", line 45, in <module>\n      ytb_ranking.fit(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/bases/tf_base.py\", line 128, in fit\n      self.build_model()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/algorithms/youtube_ranking.py\", line 211, in build_model\n      concat_embed = tf.concat(self.concat_embed, axis=1)\nNode: 'concat'\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [1,16] vs. shape[1] = [10,16]\n\t [[{{node concat}}]]\n\nOriginal stack trace for 'concat':\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n    app.start()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n    self._run_once()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n    handle._run()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_20088/4289332625.py\", line 45, in <module>\n    ytb_ranking.fit(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/bases/tf_base.py\", line 128, in fit\n    self.build_model()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/algorithms/youtube_ranking.py\", line 211, in build_model\n    concat_embed = tf.concat(self.concat_embed, axis=1)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 1650, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1278, in concat_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 3381, in _create_op_internal\n    ret = Operation.from_node_def(\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1379\u001B[0m, in \u001B[0;36mBaseSession._do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1379\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1380\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOpError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1362\u001B[0m, in \u001B[0;36mBaseSession._do_run.<locals>._run_fn\u001B[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[1;32m   1361\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extend_graph()\n\u001B[0;32m-> 1362\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_tf_sessionrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1363\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1455\u001B[0m, in \u001B[0;36mBaseSession._call_tf_sessionrun\u001B[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[1;32m   1453\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_tf_sessionrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, options, feed_dict, fetch_list, target_list,\n\u001B[1;32m   1454\u001B[0m                         run_metadata):\n\u001B[0;32m-> 1455\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRun_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1456\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mfetch_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1457\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [1,16] vs. shape[1] = [10,16]\n\t [[{{node concat}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[165], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m     df_result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m scores\n\u001B[1;32m     41\u001B[0m     display(df_result[df_result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspeaks?\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m]\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m6\u001B[39m])\n\u001B[0;32m---> 43\u001B[0m \u001B[43mfoo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mytb_ranking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[165], line 33\u001B[0m, in \u001B[0;36mfoo\u001B[0;34m(child_index, model, n_rec)\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     31\u001B[0m word_ids \u001B[38;5;241m=\u001B[39m recommendation[child_id]\n\u001B[0;32m---> 33\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchild_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mword_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m display(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-> Recommended words:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m df_result \u001B[38;5;241m=\u001B[39m df_words\u001B[38;5;241m.\u001B[39mloc[word_ids]\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/bases/tf_base.py:184\u001B[0m, in \u001B[0;36mTfBase.predict\u001B[0;34m(self, user, item, feats, cold_start, inner_id)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNCF\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m feats \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNCF can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt use features.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpredict_tf_feat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcold_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minner_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/prediction/predict.py:72\u001B[0m, in \u001B[0;36mpredict_tf_feat\u001B[0;34m(model, user, item, feats, cold_start, inner_id)\u001B[0m\n\u001B[1;32m     61\u001B[0m seqs, seq_len \u001B[38;5;241m=\u001B[39m get_cached_seqs(model, user_indices, repeat\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     62\u001B[0m feed_dict \u001B[38;5;241m=\u001B[39m get_feed_dict(\n\u001B[1;32m     63\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     64\u001B[0m     user_indices\u001B[38;5;241m=\u001B[39muser_indices,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m     is_training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     71\u001B[0m )\n\u001B[0;32m---> 72\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m normalize_prediction(preds, model, cold_start, unknown_num, unknown_index)\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:969\u001B[0m, in \u001B[0;36mBaseSession.run\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    966\u001B[0m run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    968\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 969\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions_ptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    970\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    971\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[1;32m    972\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1192\u001B[0m, in \u001B[0;36mBaseSession._run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001B[39;00m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m final_fetches \u001B[38;5;129;01mor\u001B[39;00m final_targets \u001B[38;5;129;01mor\u001B[39;00m (handle \u001B[38;5;129;01mand\u001B[39;00m feed_dict_tensor):\n\u001B[0;32m-> 1192\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_targets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_fetches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mfeed_dict_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1195\u001B[0m   results \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1372\u001B[0m, in \u001B[0;36mBaseSession._do_run\u001B[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1369\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1372\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_run_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1375\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001B[0;32m~/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/client/session.py:1398\u001B[0m, in \u001B[0;36mBaseSession._do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1393\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124monly supports NHWC tensor format\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m message:\n\u001B[1;32m   1394\u001B[0m   message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mA possible workaround: Try disabling Grappler optimizer\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1395\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mby modifying the config for creating the session eg.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1396\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124msession_config.graph_options.rewrite_options.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1397\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisable_meta_optimizer = True\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1398\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(e)(node_def, op, message)\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\nDetected at node 'concat' defined at (most recent call last):\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n      app.start()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_20088/4289332625.py\", line 45, in <module>\n      ytb_ranking.fit(\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/bases/tf_base.py\", line 128, in fit\n      self.build_model()\n    File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/algorithms/youtube_ranking.py\", line 211, in build_model\n      concat_embed = tf.concat(self.concat_embed, axis=1)\nNode: 'concat'\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [1,16] vs. shape[1] = [10,16]\n\t [[{{node concat}}]]\n\nOriginal stack trace for 'concat':\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/traitlets/config/application.py\", line 1046, in launch_instance\n    app.start()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n    self._run_once()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n    handle._run()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_20088/4289332625.py\", line 45, in <module>\n    ytb_ranking.fit(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/bases/tf_base.py\", line 128, in fit\n    self.build_model()\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/libreco/algorithms/youtube_ranking.py\", line 211, in build_model\n    concat_embed = tf.concat(self.concat_embed, axis=1)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 1176, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 1650, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1278, in concat_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/home/gustavo/.conda/envs/tici-39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 3381, in _create_op_internal\n    ret = Operation.from_node_def(\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets\n",
    "from IPython.display import display\n",
    "from typing import Dict\n",
    "\n",
    "df_children = df[[\"child_id\", \"age\"]].drop_duplicates(\"child_id\")\n",
    "df_children.set_index(\"child_id\", inplace=True)\n",
    "\n",
    "# @interact(child_index=widgets.BoundedIntText(\n",
    "#     value=0,\n",
    "#     min=0,\n",
    "#     max=len(df_children),\n",
    "#     step=1,\n",
    "#     description='Child:',\n",
    "#     disabled=False\n",
    "# ))\n",
    "def foo(child_index, model, n_rec=1):\n",
    "    child_id = df_children.index[child_index]\n",
    "    print(f\"-> Child id is: {child_id}\")\n",
    "    age = df_children.loc[child_id].age\n",
    "    print(f\"-> Child age: {age}\")\n",
    "    words = set(df[df[\"child_id\"] == child_id][\"item_definition\"].unique())\n",
    "    print(f\"-> Words spoken by this child: {len(words)}\")\n",
    "    display(\" | \".join(sorted(words)))\n",
    "    \n",
    "    for _ in range(10):\n",
    "        recommendation: Dict[int, np.ndarray] = model.recommend_user(user=child_id, n_rec=n_rec, filter_consumed=True)\n",
    "        if len(recommendation) > 1:\n",
    "            display(recommendation)\n",
    "            break\n",
    "        \n",
    "    word_ids = recommendation[child_id]\n",
    "    \n",
    "    scores = model.predict(user=child_id, item=word_ids)\n",
    "    \n",
    "    display(\"-> Recommended words:\")\n",
    "    df_result = df_words.loc[word_ids]\n",
    "    \n",
    "    # Add column to know if the child already speaks such word (from the original dataset) \n",
    "    df_result[\"speaks?\"] = df_result[\"item_definition\"].apply(lambda w: w in words)\n",
    "    df_result[\"score\"] = scores\n",
    "    display(df_result[df_result[\"speaks?\"] == False].iloc[:6])\n",
    "    \n",
    "foo(5, ytb_ranking, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff646a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from libreco.data import DataInfo\n",
    "\n",
    "def predict(words: List[str]):\n",
    "    child_id = -1\n",
    "    # Train the model with the words spoken by the new child\n",
    "    words_ids = df_words[df_words[\"item_definition\"].isin(words)].index.tolist()\n",
    "    df_train = pd.DataFrame({\"user\": child_id, \"item\": words_ids, \"label\": 1.0})\n",
    "    \n",
    "    old_data_info = DataInfo.load(\"models//data-info\", model_name=\"label-is-all-1-epochs=3\")\n",
    "    data, data_info = DatasetFeat.merge_trainset(df_train, old_data_info)\n",
    "    model = LightGCN(\n",
    "        task=\"ranking\",\n",
    "        data_info=data_info,\n",
    "        loss_type=\"bpr\",\n",
    "        embed_size=16,\n",
    "        n_epochs=1,\n",
    "        lr=1e-3,\n",
    "        batch_size=2048,\n",
    "        num_neg=1,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    model.rebuild_model(\"models/lightgcn-retrain/weights\", model_name=\"label-is-all-1-epochs=3\")\n",
    "    \n",
    "    model.fit(\n",
    "        data,\n",
    "        neg_sampling=True,  # sample negative items for train and eval data\n",
    "        verbose=2,\n",
    "    )\n",
    "    \n",
    "    # Predict the words for this child\n",
    "    recommendation: Dict[int, np.ndarray] = lightgcn.recommend_user(user=child_id, n_rec=100)\n",
    "    predicted_word_ids = recommendation[child_id]\n",
    "    scores = lightgcn.predict(user=child_id, item=predicted_word_ids)\n",
    "    \n",
    "    df_result = df_words.loc[predicted_word_ids]\n",
    "    df_result[\"score\"] = scores\n",
    "    df_result[\"speaks?\"] = df_result[\"item_definition\"].apply(lambda w: w in words)\n",
    "    display(df_result[~df_result[\"speaks?\"]].iloc[:6])\n",
    "    \n",
    "predict(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
