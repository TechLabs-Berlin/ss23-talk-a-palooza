{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MS24e5p5cCitfK_EzmawgSFsWArUh0GE","timestamp":1697405238326}],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyMsNT7RR3CVgYfAMoV+xDS0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"PeLX-UWe249A","executionInfo":{"status":"ok","timestamp":1697405725347,"user_tz":-120,"elapsed":2,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib as plt\n","import re\n","import shutil\n","import os\n","import math, random\n","import torch\n","import torchaudio\n","from torchaudio import transforms\n","from IPython.display import Audio\n","from fastcore.utils import gt\n","from pathlib import Path"],"metadata":{"id":"RTZJEfas4B8o","executionInfo":{"status":"ok","timestamp":1697405725589,"user_tz":-120,"elapsed":3,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXyaAbHW4R9o","executionInfo":{"status":"ok","timestamp":1697405747592,"user_tz":-120,"elapsed":22006,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"7e19c9a4-7080-4ebf-e49b-3a7d3e6595cf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Talkapalooza - TL BL SS 23"],"metadata":{"id":"WgcBRbGdLfb5"}},{"cell_type":"markdown","source":["# A custom Pytorch CNN:"],"metadata":{"id":"SaFU2yCwLaZf"}},{"cell_type":"markdown","source":["In hopes of getting better results and to better understand the purviews of deep learning, I have researched examples for the architecture of convolutional neural networks used in audio recognition."],"metadata":{"id":"bW16-Q2cPR2a"}},{"cell_type":"markdown","source":["## Preparing audio files and data frames"],"metadata":{"id":"NAVh57YgKhMA"}},{"cell_type":"markdown","source":["As per my previous experiences, I move ahead with a dataset filtering out recordings that have less than 5 recordings."],"metadata":{"id":"UqXfNsnjPzbV"}},{"cell_type":"code","source":["src = '/content/drive/MyDrive/talkapalooza/labelled_audio/'\n","audio_data = Path(src).ls()\n","audio_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHW7tVmD4a41","executionInfo":{"status":"ok","timestamp":1697405750067,"user_tz":-120,"elapsed":2481,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"5d71c7ef-ddec-4b38-85c3-ee17cb758a1a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#471) [Path('/content/drive/MyDrive/talkapalooza/labelled_audio/yellow'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/big'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/nose'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/where'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/hear'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/hair'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/vacuum'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/bedroom'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/picture'),Path('/content/drive/MyDrive/talkapalooza/labelled_audio/above')...]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["filtered_wordbank_df = pd.read_csv('/content/drive/MyDrive/talkapalooza/data/wordbank_crossref_filt3.csv')"],"metadata":{"id":"PiIj7JkN5tiH","executionInfo":{"status":"ok","timestamp":1697405750451,"user_tz":-120,"elapsed":387,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["filtered_words_5 = filtered_wordbank_df[filtered_wordbank_df[\"count\"] >= 5][\"word\"]\n","filtered_paths_5 = list()"],"metadata":{"id":"1Us3w0Q-5z-p","executionInfo":{"status":"ok","timestamp":1697405750751,"user_tz":-120,"elapsed":6,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for p in audio_data:\n","  if p.name.split('/')[-1] in list(filtered_words_5):\n","    filtered_paths_5.append(p)"],"metadata":{"id":"QeeBUI6t56_D","executionInfo":{"status":"ok","timestamp":1697405750752,"user_tz":-120,"elapsed":7,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["len(filtered_paths_5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxnkpf4K6Sqx","executionInfo":{"status":"ok","timestamp":1697405750752,"user_tz":-120,"elapsed":6,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"c0121748-fe93-4596-b216-1a1c1027423c"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["53"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### Creating the meta and classes dataframes"],"metadata":{"id":"q0vHEiCZQQLM"}},{"cell_type":"markdown","source":["The meta dataframe stores the paths and class ID of each recording. The class dataframe has information on which word each class ID represents."],"metadata":{"id":"sMdHF2-nQXii"}},{"cell_type":"code","source":["meta_df = pd.DataFrame(columns={'path': [], 'class_id': []})"],"metadata":{"id":"P1gD_Hsg-6az","executionInfo":{"status":"ok","timestamp":1697405750752,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class_df = pd.DataFrame(columns={'class_id': [], 'class_name': []})"],"metadata":{"id":"pYSJI4dq8Rep","executionInfo":{"status":"ok","timestamp":1697405750753,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class_id = 0\n","meta_id = 0\n","for p in filtered_paths_5:\n","  class_df.loc[class_id] = class_id, p.name\n","  for path, subdirs, files in os.walk(p):\n","    i = 0\n","    for file in files:\n","        p_src = os.path.abspath(Path(path, file))\n","        p_dst ='/content/drive/MyDrive/talkapalooza/labelled_audio_filter5/' + str(class_id) + '-' + str(i) + '.ogg'\n","        meta_df.loc[meta_id] = p_dst, class_id\n","        # shutil.copy(p_src, p_dst)\n","        i += 1\n","        meta_id += 1\n","  class_id += 1"],"metadata":{"id":"KXD7lYqY6k8S","executionInfo":{"status":"ok","timestamp":1697405761960,"user_tz":-120,"elapsed":11212,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ux1_9F6u-Msb","executionInfo":{"status":"ok","timestamp":1697405761961,"user_tz":-120,"elapsed":19,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"d88744aa-927f-4b4e-ed8e-e66e7b02488c"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   class_id class_name\n","0         0        big\n","1         1      where\n","2         2       foot\n","3         3       with\n","4         4      stick"],"text/html":["\n","  <div id=\"df-a155409c-3d84-44a6-a04b-405b40dac810\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class_id</th>\n","      <th>class_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>big</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>where</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>foot</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>with</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>stick</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a155409c-3d84-44a6-a04b-405b40dac810')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a155409c-3d84-44a6-a04b-405b40dac810 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a155409c-3d84-44a6-a04b-405b40dac810');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cdc7af73-e110-47f9-829a-d77aa919606c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cdc7af73-e110-47f9-829a-d77aa919606c')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cdc7af73-e110-47f9-829a-d77aa919606c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["meta_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"zY5FpJOC_f1k","executionInfo":{"status":"ok","timestamp":1697405761961,"user_tz":-120,"elapsed":17,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"70fc4b71-5b3f-4de8-da96-e64434bd42a0"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                path  class_id\n","0  /content/drive/MyDrive/talkapalooza/labelled_a...         0\n","1  /content/drive/MyDrive/talkapalooza/labelled_a...         0\n","2  /content/drive/MyDrive/talkapalooza/labelled_a...         0\n","3  /content/drive/MyDrive/talkapalooza/labelled_a...         0\n","4  /content/drive/MyDrive/talkapalooza/labelled_a...         0"],"text/html":["\n","  <div id=\"df-b27fc39b-05d6-469e-889f-78fdb86dd9a2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>class_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/content/drive/MyDrive/talkapalooza/labelled_a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/content/drive/MyDrive/talkapalooza/labelled_a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/content/drive/MyDrive/talkapalooza/labelled_a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/content/drive/MyDrive/talkapalooza/labelled_a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/content/drive/MyDrive/talkapalooza/labelled_a...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b27fc39b-05d6-469e-889f-78fdb86dd9a2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b27fc39b-05d6-469e-889f-78fdb86dd9a2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b27fc39b-05d6-469e-889f-78fdb86dd9a2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5b73706e-38fe-44d9-a1d9-ff033873ee06\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b73706e-38fe-44d9-a1d9-ff033873ee06')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5b73706e-38fe-44d9-a1d9-ff033873ee06 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Setting up audio preprocessing"],"metadata":{"id":"xVQAih2iKkPi"}},{"cell_type":"markdown","source":["### Preparing AudioUtil class with methods to load, convert audio and more"],"metadata":{"id":"VeXIENSGKur0"}},{"cell_type":"markdown","source":["Adapted from [Audio Deep Learning by Ketan Doshi](https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5), including the creation of an AudioUtil class that houses several methods that allow for the transformation and augmentation of loaded audio data."],"metadata":{"id":"vcvNRR4mMrN0"}},{"cell_type":"code","source":["class AudioUtil():\n","  # Open audio file using torchaudio\n","  @staticmethod\n","  def open(af):\n","    x, sr = torchaudio.load(af)\n","    return (x, sr)\n","\n","  # Convert to desired num of channels\n","  @staticmethod\n","  def rechannel(aud, new_channel):\n","    x, sr = aud\n","\n","    if (x.shape[0] == new_channel):\n","      return aud\n","\n","    if (new_channel == 1):\n","      rex = x[:1, :]\n","    else:\n","      rex = torch.cat([x, x])\n","\n","    return ((rex, sr))\n","\n","  # Convert to desired sampling rate\n","  @staticmethod\n","  def resample(aud, newsr):\n","    x, sr = aud\n","\n","    if (sr == newsr):\n","      return aud\n","\n","    num_channels = x.shape[0]\n","    rex = torchaudio.transforms.Resample(sr, newsr)(x[:1,:])\n","    if (num_channels > 1):\n","      retwo = torchaudio.transforms.Resample(sr, newsr)(x[1:,:])\n","      rex = torch.cat([rex, retwo])\n","\n","    return ((rex, newsr))\n","\n","  # Resize audio to desired size by either truncating or padding with silence\n","  @staticmethod\n","  def resize(aud, max_ms):\n","    x, sr = aud\n","    num_rows, x_len = x.shape\n","    max_len = sr//1000 * max_ms\n","\n","    if (x_len > max_len):\n","      x = x[:,:max_len]\n","\n","    elif (x_len < max_len):\n","      pad_begin_len = random.randint(0, max_len - x_len)\n","      pad_end_len = max_len - x_len - pad_begin_len\n","\n","      pad_begin = torch.zeros((num_rows, pad_begin_len))\n","      pad_end = torch.zeros((num_rows, pad_end_len))\n","\n","      x = torch.cat((pad_begin, x, pad_end), 1)\n","\n","    return (x, sr)\n","\n","  # Data augmentation: shifting audio left or right randomly\n","  @staticmethod\n","  def time_shift(aud, max_shift):\n","    x,sr = aud\n","    _, x_len = x.shape\n","    shift_amt = int(random.random() * max_shift * x_len)\n","    return (x.roll(shift_amt), sr)\n","\n","  # Create a spectrogram\n","  @staticmethod\n","  def to_spectro(aud, n_mels=64, n_fft=1024, hop_len=None):\n","    x,sr = aud\n","    top_db = 80\n","\n","    spec = transforms.MFCC(sr, melkwargs={\n","        'n_fft': n_fft,\n","        'hop_length': hop_len,\n","        'n_mels': n_mels})(x)\n","\n","    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","    return (spec)\n","\n","  # Data augmentation: masking time||frequency with vert/horiz bars\n","  @staticmethod\n","  def mask_spectro(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","    _, n_mels, n_steps = spec.shape\n","    mask_value = spec.mean()\n","    aug_spec = spec\n","\n","    freq_mask_param = max_mask_pct * n_mels\n","    for _ in range(n_freq_masks):\n","      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","\n","    time_mask_param = max_mask_pct * n_steps\n","    for _ in range(n_time_masks):\n","      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","\n","    return aug_spec"],"metadata":{"id":"pkMPbML2K99n","executionInfo":{"status":"ok","timestamp":1697405761962,"user_tz":-120,"elapsed":17,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Create custom data loader"],"metadata":{"id":"a3arOvLqQntn"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio"],"metadata":{"id":"jc_BDbTHQrpd","executionInfo":{"status":"ok","timestamp":1697405761962,"user_tz":-120,"elapsed":15,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class WordDS(Dataset):\n","  def __init__(self, df):\n","    self.df = df\n","    # self.data_path = str(data_path)\n","    self.duration = 2000\n","    self.sr = 44100\n","    self.channel = 2\n","    self.shift_pct = 0.4\n","\n","  # Number of items\n","  def __len__(self):\n","    return len(self.df)\n","\n","  # Get item at index\n","  def __getitem__(self, idx):\n","    audio_file = self.df.loc[idx, 'path']\n","    # audio_file = self.data_path + self.df.loc[idx, 'path']\n","    class_id = self.df.loc[idx, 'class_id']\n","\n","    aud = AudioUtil.open(audio_file)\n","    reaud = AudioUtil.resample(aud, self.sr)\n","    rechan = AudioUtil.rechannel(reaud, self.channel)\n","\n","    dur_aud = AudioUtil.resize(rechan, self.duration)\n","    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n","    sgram = AudioUtil.to_spectro(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n","    aug_sgram = AudioUtil.mask_spectro(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","    return aug_sgram, class_id"],"metadata":{"id":"7ycSnxEuQyOP","executionInfo":{"status":"ok","timestamp":1697405761963,"user_tz":-120,"elapsed":16,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Split data into batches"],"metadata":{"id":"nWopJWwfRgsv"}},{"cell_type":"code","source":["from torch.utils.data import random_split"],"metadata":{"id":"z6-rfqALRofm","executionInfo":{"status":"ok","timestamp":1697405761963,"user_tz":-120,"elapsed":15,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["words_ds = WordDS(meta_df)\n","\n","# Random split of 80:20 between training and validation\n","num_items = len(words_ds)\n","num_train = round(num_items * 0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(words_ds, [num_train, num_val])\n","\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"],"metadata":{"id":"UEiXYIJcRm0m","executionInfo":{"status":"ok","timestamp":1697405761963,"user_tz":-120,"elapsed":14,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## Creating the model"],"metadata":{"id":"VX2HCP9vVZn3"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import init\n","\n","class AudioClassifier (nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Third Convolution Block\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Fourth Convolution Block\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(128)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=128, out_features=53)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","\n","    # Forward pass computations\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJ6d_EtXVWEq","executionInfo":{"status":"ok","timestamp":1697405824860,"user_tz":-120,"elapsed":5408,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"96f5922e-7594-401c-cf8f-3db61a107d47"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"DT2HuVvdV9eU"}},{"cell_type":"code","source":["def training(model, train_dl, num_epochs):\n","  # Loss Function, Optimizer and Scheduler\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.03)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.03,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","  # Repeat for each epoch\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Normalize the inputs\n","        inputs_m, inputs_s = inputs.mean(), inputs.std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","\n","    # Print stats at the end of the epoch\n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n","\n","  print('Finished Training')"],"metadata":{"id":"IdLqY4xwV--g","executionInfo":{"status":"ok","timestamp":1697405839234,"user_tz":-120,"elapsed":224,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["num_epochs=20\n","training(myModel, train_dl, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sch0EmccWpx","executionInfo":{"status":"ok","timestamp":1696956230641,"user_tz":-120,"elapsed":84129,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"50dca5be-3f96-4a94-e2da-91bf87cb47f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 4.09, Accuracy: 0.02\n","Epoch: 1, Loss: 4.14, Accuracy: 0.02\n","Epoch: 2, Loss: 4.07, Accuracy: 0.02\n","Epoch: 3, Loss: 4.15, Accuracy: 0.02\n","Epoch: 4, Loss: 4.16, Accuracy: 0.02\n","Epoch: 5, Loss: 4.13, Accuracy: 0.03\n","Epoch: 6, Loss: 4.27, Accuracy: 0.03\n","Epoch: 7, Loss: 4.09, Accuracy: 0.04\n","Epoch: 8, Loss: 4.00, Accuracy: 0.04\n","Epoch: 9, Loss: 3.87, Accuracy: 0.04\n","Epoch: 10, Loss: 3.82, Accuracy: 0.05\n","Epoch: 11, Loss: 3.72, Accuracy: 0.07\n","Epoch: 12, Loss: 3.69, Accuracy: 0.05\n","Epoch: 13, Loss: 3.58, Accuracy: 0.08\n","Epoch: 14, Loss: 3.57, Accuracy: 0.09\n","Epoch: 15, Loss: 3.54, Accuracy: 0.09\n","Epoch: 16, Loss: 3.41, Accuracy: 0.12\n","Epoch: 17, Loss: 3.57, Accuracy: 0.08\n","Epoch: 18, Loss: 3.37, Accuracy: 0.12\n","Epoch: 19, Loss: 3.40, Accuracy: 0.14\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["### Modifying Kernel size"],"metadata":{"id":"TuEaKXseSvAz"}},{"cell_type":"markdown","source":["I tried a variety of setups to gauge the impact of kernel size, stride etc. at different layers of the CNN."],"metadata":{"id":"NFNQHtheS3Rx"}},{"cell_type":"code","source":["class AudioClassifier (nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 16, kernel_size=(20, 20), stride=(6, 6), padding=(10, 10))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Third Convolution Block\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Fourth Convolution Block\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(128)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=128, out_features=53)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","\n","    # Forward pass computations\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJM17UHfdOyr","executionInfo":{"status":"ok","timestamp":1696955987865,"user_tz":-120,"elapsed":294,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"1a2ac21e-d54a-45f1-b23a-0ecb05a8e387"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["num_epochs=20\n","training(myModel, train_dl, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtM-aH36evLo","executionInfo":{"status":"ok","timestamp":1697404727323,"user_tz":-120,"elapsed":167123,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"e22013ae-e88f-4fcb-8246-7dc74fda0ec4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 4.02, Accuracy: 0.03\n","Epoch: 1, Loss: 3.98, Accuracy: 0.01\n","Epoch: 2, Loss: 3.87, Accuracy: 0.05\n","Epoch: 3, Loss: 3.76, Accuracy: 0.05\n","Epoch: 4, Loss: 3.62, Accuracy: 0.08\n","Epoch: 5, Loss: 3.56, Accuracy: 0.08\n","Epoch: 6, Loss: 3.47, Accuracy: 0.08\n","Epoch: 7, Loss: 3.36, Accuracy: 0.12\n","Epoch: 8, Loss: 3.10, Accuracy: 0.13\n","Epoch: 9, Loss: 2.91, Accuracy: 0.19\n","Epoch: 10, Loss: 2.75, Accuracy: 0.23\n","Epoch: 11, Loss: 2.67, Accuracy: 0.27\n","Epoch: 12, Loss: 2.36, Accuracy: 0.36\n","Epoch: 13, Loss: 2.39, Accuracy: 0.36\n","Epoch: 14, Loss: 2.16, Accuracy: 0.38\n","Epoch: 15, Loss: 2.03, Accuracy: 0.39\n","Epoch: 16, Loss: 1.96, Accuracy: 0.44\n","Epoch: 17, Loss: 1.75, Accuracy: 0.50\n","Epoch: 18, Loss: 1.73, Accuracy: 0.52\n","Epoch: 19, Loss: 1.64, Accuracy: 0.52\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["Returning to a small kernel size, but increasing the stride throughout helped increase the accuracy dramatically:"],"metadata":{"id":"HcWtFakvNcf5"}},{"cell_type":"code","source":["class AudioClassifier (nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(2, 16, kernel_size=(5, 5), stride=(3, 3), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Third Convolution Block\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Fourth Convolution Block\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(128)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=128, out_features=53)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","\n","    # Forward pass computations\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8V5LyKLMQu_I","executionInfo":{"status":"ok","timestamp":1697406018021,"user_tz":-120,"elapsed":254,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"e8591faa-3869-44fc-d761-ef2e49b22c82"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["num_epochs=20\n","training(myModel, train_dl, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoTKp4V-Rd3c","executionInfo":{"status":"ok","timestamp":1697406084817,"user_tz":-120,"elapsed":57450,"user":{"displayName":"Alyosha","userId":"08073617009990359039"}},"outputId":"41f0653c-951d-4c24-de64-17e4dbf29cc1"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 3.97, Accuracy: 0.02\n","Epoch: 1, Loss: 3.87, Accuracy: 0.04\n","Epoch: 2, Loss: 3.70, Accuracy: 0.07\n","Epoch: 3, Loss: 3.58, Accuracy: 0.07\n","Epoch: 4, Loss: 3.41, Accuracy: 0.09\n","Epoch: 5, Loss: 3.41, Accuracy: 0.09\n","Epoch: 6, Loss: 3.36, Accuracy: 0.11\n","Epoch: 7, Loss: 3.10, Accuracy: 0.17\n","Epoch: 8, Loss: 2.82, Accuracy: 0.19\n","Epoch: 9, Loss: 2.77, Accuracy: 0.22\n","Epoch: 10, Loss: 2.30, Accuracy: 0.37\n","Epoch: 11, Loss: 2.19, Accuracy: 0.37\n","Epoch: 12, Loss: 1.91, Accuracy: 0.41\n","Epoch: 13, Loss: 1.79, Accuracy: 0.49\n","Epoch: 14, Loss: 1.36, Accuracy: 0.64\n","Epoch: 15, Loss: 1.33, Accuracy: 0.58\n","Epoch: 16, Loss: 1.16, Accuracy: 0.69\n","Epoch: 17, Loss: 1.02, Accuracy: 0.77\n","Epoch: 18, Loss: 0.94, Accuracy: 0.73\n","Epoch: 19, Loss: 0.88, Accuracy: 0.78\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["## Closing thoughts"],"metadata":{"id":"Gd81xVx4TdCQ"}},{"cell_type":"markdown","source":["### Room for further improvement\n","\n","I identified four remaining key avenues for future improvements:\n","\n","1. Utility functions: Fine-tune utility functions responsible for audio data augmentation, including custom transformations and diverse augmentation ranges.\n","2. Neural network architecture: Further exploration and modification of the network's architecture.\n","3. Neural network parameters: Experimentation with learning rate optimizers, batch sizes, train/test splits and other parameters.\n","4. The underlying dataset: A thorough examination of the dataset to assess potential improvements, including the impact of restricting training to words with a minimum of eight recordings (instead of the current five). Collecting more training data is also critical."],"metadata":{"id":"ddOyNWECTkxl"}},{"cell_type":"markdown","source":["### Looking past the model\n","\n","As the second model is being refined, the API needs to be revisited, as well. The intelligibility score in particular needs to be reassessed. Considering the possible differences in dialect, accent, and speaker's native tongue relative to the training data negatively impact the intelligibility score as it is currently implemented. The score should serve as a general indicator of intelligibility rather than a definitive marker of success or failure. This also affects how the score is presented on the frontend, of course."],"metadata":{"id":"kp6AfwnITtRz"}}]}